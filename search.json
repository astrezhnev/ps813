[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "A schedule of topics and readings is provided below. Each week will cover a single topic or group of topics. Monday lectures will typically be an introduction to the topic while Wednesday lectures will go into greater detail and involve some applications of the method. You should make sure to review the readings prior to that week’s lectures with an aim towards completing the reading assignments prior to Wednesday’s lecture.\nApplications are optional but useful examples of a given methodological approach implemented in a published political science (or adjacent discipline) paper. I have tried to curate two examples for each week that provide a contrast between an “older” vs. a more “modern” paper using a particular methodology, highlight a debate over an interesting empirical question that hinges on a methodological problem, and/or just generally provide a cool empirical application using the framework being taught that week.\nAll hyperlinks to papers are either to the published version of the paper (if published) or to the working paper. You should use your university library access to obtain the full version if the article is not open access. Likewise, textbook readings will be to the free/open version of the book (if available) or to the UW-Madison university library eBook."
  },
  {
    "objectID": "schedule.html#week-1-january-21-statistical-review",
    "href": "schedule.html#week-1-january-21-statistical-review",
    "title": "Schedule",
    "section": "Week 1: January 21 | Statistical Review",
    "text": "Week 1: January 21 | Statistical Review\nTopics:\n\nEstimands and estimators\nReview of statistical properties of estimators (bias/variance)\nLarge sample theory, big-O/little-o notation\n\nReadings:\n\nChapter 1, Imbens and Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press. 2015.\nChapter 1, Hernán and Robins. Causal Inference: What If. Chapman & Hall/CRC. 2020.\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. “What is your estimand? Defining the target quantity connects statistical evidence to theory.” American Sociological Review 86.3 (2021): 532-565.\nTorreblanca, Carolina, William Dinneen, Guy Grossman, and Yiqing Xu. “The Credibility Revolution in Political Science.” Available at SSRN 5842482 (2025)."
  },
  {
    "objectID": "schedule.html#week-2-january-26-28-potential-outcomes-randomized-experiments",
    "href": "schedule.html#week-2-january-26-28-potential-outcomes-randomized-experiments",
    "title": "Schedule",
    "section": "Week 2: January 26, 28 | Potential Outcomes + Randomized Experiments",
    "text": "Week 2: January 26, 28 | Potential Outcomes + Randomized Experiments\nMonday, January 26 & Wednesday, January 28\nTopics:\n\nCounterfactual reasoning and the potential outcomes model\nWhat assumptions are needed to identify average treatment effects\nWhy randomized experiments satisfy these assumptions\nEstimation and randomization inference in standard experimental designs\n\nReadings:\n\nSections 1-5, Athey and Imbens, “The Econometrics of Randomized Experiments,” Handbook of economic field experiments. Vol. 1. North-Holland, 2017. 73-140.\nChapter 2, Hernán and Robins. Causal Inference: What If. Chapman & Hall/CRC. 2020.\nDruckman, James N., et al. “The growth and development of experimental research in political science.” American Political Science Review 100.4 (2006): 627-635.\n\nApplications:\n\nGerber, Alan S., Donald P. Green, and Christopher W. Larimer. “Social pressure and voter turnout: Evidence from a large-scale field experiment.” American Political Science Review 102.1 (2008): 33-48.\nBroockman, David E., and Joshua L. Kalla. “Consuming cross-cutting media causes learning and moderates attitudes: A field experiment with Fox News viewers.” The Journal of Politics 87, no. 1 (2025): 246-261."
  },
  {
    "objectID": "schedule.html#week-3-february-2-6-experiments-incorporating-covariates-and-statistical-power",
    "href": "schedule.html#week-3-february-2-6-experiments-incorporating-covariates-and-statistical-power",
    "title": "Schedule",
    "section": "Week 3: February 2-6 | Experiments: Incorporating Covariates and Statistical Power",
    "text": "Week 3: February 2-6 | Experiments: Incorporating Covariates and Statistical Power\nMonday, February 2 & Wednesday, February 4\nTopics:\n\nStratification/blocking and using covariates in experiments\nAnalysis of cluster-randomized experiments\nPower, precision and Type II error.\n\nReadings:\n\nSections 6-12, Athey and Imbens, “The Econometrics of Randomized Experiments,” Handbook of economic field experiments. Vol. 1. North-Holland, 2017. 73-140.\nLin, Winston. “Agnostic notes on regression adjustments to experimental data: Reexamining Freedman’s critique.” The Annals of Applied Statistics 7.1 (2013): 295-318.\n\nBonus: Samii, C., and P. M. Aronow. “On equivalencies between design-based and regression-based variance estimators for randomized experiments.” Statistics & Probability Letters 82.2 (2012): 365-370.\n\nGelman, Andrew, and John Carlin. “Beyond power calculations: Assessing type S (sign) and type M (magnitude) errors.” Perspectives on psychological science 9, no. 6 (2014): 641-651.\n\nApplications:\n\nCasey, K., Glennerster, R., & Miguel, E. (2012). Reshaping institutions: Evidence on aid impacts using a preanalysis plan. The Quarterly Journal of Economics, 127(4), 1755-1812.\nCrépon, B., Devoto, F., Duflo, E., & Parienté, W. (2015). Estimating the impact of microcredit on those who take it up: Evidence from a randomized experiment in Morocco. American Economic Journal: Applied Economics, 7(1), 123-50."
  },
  {
    "objectID": "schedule.html#week-4-february-9-13-experiments-attrition-and-generalizability",
    "href": "schedule.html#week-4-february-9-13-experiments-attrition-and-generalizability",
    "title": "Schedule",
    "section": "Week 4: February 9-13 | Experiments: Attrition and Generalizability",
    "text": "Week 4: February 9-13 | Experiments: Attrition and Generalizability\nMonday, February 9 & Wednesday, February 11\nTopics:\n\nWhat happens when units drop out of the study?\nWhat sorts of covariates should we not adjust for in an experiment?\nHow do we generalize from one experiment to other settings?\n\nReadings:\n\nMontgomery, Jacob M., Brendan Nyhan, and Michelle Torres. “How conditioning on posttreatment variables can ruin your experiment and what to do about it.” American Journal of Political Science 62.3 (2018): 760-775.\nAronow, P. M., Jonathon Baron, and Lauren Pinson. “A note on dropping experimental subjects who fail a manipulation check.” Political Analysis 27.4 (2019): 572-589.\nEgami, Naoki, and Erin Hartman. “Elements of external validity: Framework, design, and analysis.” American Political Science Review 117, no. 3 (2023): 1070-1088.\n\nApplications:\n\nGerber, Alan S., and Donald P. Green. “The effects of canvassing, telephone calls, and direct mail on voter turnout: A field experiment.” American Political Science Review 94.3 (2000): 653-663."
  },
  {
    "objectID": "schedule.html#week-5-february-16-20-selection-on-observables",
    "href": "schedule.html#week-5-february-16-20-selection-on-observables",
    "title": "Schedule",
    "section": "Week 5: February 16-20 | Selection-on-Observables",
    "text": "Week 5: February 16-20 | Selection-on-Observables\nMonday, February 16 & Wednesday, February 18\nTopics:\n\nWhat to do when random assignment of treatment is not possible – common challenges of observational designs\nAssumptions behind “no unobserved confounding” designs\nRepresenting assumptions using graphical models\nCovariate adjustment via subclassification\n\nReadings:\n\nChapter 12. Imbens and Rubin.\nChapters 6-8. Huntington-Klein. The Effect: An introduction to research design and causality. Chapman and Hall/CRC, 2021.\nChapters 3, 6-8. Hernán and Robins.\n\nApplications:\n\nWashington, Ebonya L. “Female socialization: how daughters affect their legislator fathers.” American Economic Review 98, no. 1 (2008): 311-32.\nBa, Bocar A., Dean Knox, Jonathan Mummolo, and Roman Rivera. “The role of officer race and gender in police-civilian interactions in Chicago.” Science 371, no. 6530 (2021): 696-702."
  },
  {
    "objectID": "schedule.html#week-6-february-23-27-selection-on-observables-continued",
    "href": "schedule.html#week-6-february-23-27-selection-on-observables-continued",
    "title": "Schedule",
    "section": "Week 6: February 23-27 | Selection-on-Observables Continued",
    "text": "Week 6: February 23-27 | Selection-on-Observables Continued\nMonday, February 23 & Wednesday, February 25\nTopics:\n\nPropensity scores and covariate adjustment via weighting\nMatching estimators\nRegression estimators and “doubly-robust” estimators\n\nReadings:\n\nChapter 13. Imbens and Rubin.\nImbens, G. W. (2004). Nonparametric estimation of average treatment effects under exogeneity: A review. Review of Economics and Statistics, 86(1), 4-29.\nAronow, Peter M., and Cyrus Samii. “Does regression produce representative estimates of causal effects?” American Journal of Political Science 60.1 (2016): 250-267.\nGlynn, Adam N., and Kevin M. Quinn. “An introduction to the augmented inverse propensity weighted estimator.” Political Analysis 18.1 (2010): 36-56.\nAbadie, A., & Imbens, G. W. (2011). Bias-corrected matching estimators for average treatment effects. Journal of Business & Economic Statistics, 29(1), 1-11."
  },
  {
    "objectID": "schedule.html#week-7-march-2-6-selection-on-observables-continued",
    "href": "schedule.html#week-7-march-2-6-selection-on-observables-continued",
    "title": "Schedule",
    "section": "Week 7: March 2-6 | Selection-on-Observables Continued",
    "text": "Week 7: March 2-6 | Selection-on-Observables Continued\nMonday, March 2 & Wednesday, March 4\nMidterm exam\nTopics:\n\nTBA\n\nReadings:\n\nTBA"
  },
  {
    "objectID": "schedule.html#week-8-march-9-13-instrumental-variables",
    "href": "schedule.html#week-8-march-9-13-instrumental-variables",
    "title": "Schedule",
    "section": "Week 8: March 9-13 | Instrumental Variables",
    "text": "Week 8: March 9-13 | Instrumental Variables\nMonday, March 9 & Wednesday, March 11\nTopics:\n\nEstimating effects under unobserved confounding using exogenous variation in treatment induced by an instrument\nAssumptions behind the instrumental variable strategy – exogeneity, relevance, “exclusion restriction”\nEstimation via the Wald Estimator and Two-Stage Least Squares\nInterpreting the IV estimand – Local Average Treatment Effect\nWhat makes a good instrument?\n\nReadings:\n\nCunningham, Causal Inference: The Mixtape, Chapter 7 - Instrumental Variables\nAngrist, Imbens and Rubin (1996) “Identification of causal effects using instrumental variables.” Journal of the American Statistical Association, 91:434, 444-455\nSovey, Allison J., and Donald P. Green. “Instrumental variables estimation in political science: A readers’ guide.” American Journal of Political Science 55, no. 1 (2011): 188-200.\nAndrews, Isaiah, James H. Stock, and Liyang Sun. “Weak instruments in instrumental variables regression: Theory and practice.” Annual Review of Economics 11 (2019): 727-753.\n\nApplications:\n\nDobbie, Will, Jacob Goldin, and Crystal S. Yang. “The effects of pretrial detention on conviction, future crime, and employment: Evidence from randomly assigned judges.” American Economic Review 108.2 (2018): 201-40.\nAngrist, Joshua D., and Peter Hull. “Instrumental variables methods reconcile intention-to-screen effects across pragmatic cancer screening trials.” Proceedings of the national academy of sciences 120, no. 51 (2023): e2311556120."
  },
  {
    "objectID": "schedule.html#week-9-march-16-20-differences-in-differences",
    "href": "schedule.html#week-9-march-16-20-differences-in-differences",
    "title": "Schedule",
    "section": "Week 9: March 16-20 | Differences-in-Differences",
    "text": "Week 9: March 16-20 | Differences-in-Differences\nMonday, March 16 & Wednesday, March 18\nTopics:\n\nWeakening “selection on observables” by studying changes over time\nAssumptions behind the “differences-in-differences” strategy – parallel trends\nEstimation and diagnostics for the identification assumptions\nPitfalls and challenges when units initiate treatment at different times\n\nReadings:\n\nCunningham, The Causal Inference Mixtape, Chapter 9 - Differences-in-differences\nRoth, J., Sant’Anna, P. H., Bilinski, A., & Poe, J. (2022). What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature. arXiv preprint arXiv:2201.01194.\nImai, Kosuke, In Song Kim, and Erik H. Wang. “Matching Methods for Causal Inference with Time-Series Cross-Sectional Data.” American Journal of Political Science (2021).\n\nApplications:\n\nMalesky, E. J., Nguyen, C. V., & Tran, A. (2014). The impact of recentralization on public services: A difference-in-differences analysis of the abolition of elected councils in Vietnam. American Political Science Review, 108(1), 144-168.\nMiller, S., Johnson, N., & Wherry, L. R. (2021). Medicaid and mortality: new evidence from linked survey and administrative data. The Quarterly Journal of Economics, 136(3), 1783-1829."
  },
  {
    "objectID": "schedule.html#week-10-march-23-27-differences-in-differences-continued",
    "href": "schedule.html#week-10-march-23-27-differences-in-differences-continued",
    "title": "Schedule",
    "section": "Week 10: March 23-27 | Differences-in-Differences Continued",
    "text": "Week 10: March 23-27 | Differences-in-Differences Continued\nMonday, March 23 & Wednesday, March 25\nTopics:\n\nTBA\n\nReadings:\n\nTBA"
  },
  {
    "objectID": "schedule.html#spring-break-march-28---april-5",
    "href": "schedule.html#spring-break-march-28---april-5",
    "title": "Schedule",
    "section": "Spring Break: March 28 - April 5",
    "text": "Spring Break: March 28 - April 5\nNo class"
  },
  {
    "objectID": "schedule.html#week-11-april-6-10-panel-data-causal-inference",
    "href": "schedule.html#week-11-april-6-10-panel-data-causal-inference",
    "title": "Schedule",
    "section": "Week 11: April 6-10 | Panel Data Causal Inference",
    "text": "Week 11: April 6-10 | Panel Data Causal Inference\nMonday, April 6 & Wednesday, April 8\nTopics:\n\nTBA\n\nReadings:\n\nTBA"
  },
  {
    "objectID": "schedule.html#week-12-april-13-17-regression-discontinuity-designs",
    "href": "schedule.html#week-12-april-13-17-regression-discontinuity-designs",
    "title": "Schedule",
    "section": "Week 12: April 13-17 | Regression Discontinuity Designs",
    "text": "Week 12: April 13-17 | Regression Discontinuity Designs\nMonday, April 13 & Wednesday, April 15\nTopics:\n\nEstimating effects under unobserved confounding using quasi-random assignment at a cutpoint\nCommon applications: Elections, test scores\nEstimation and sensitivity to modeling assumptions\n\nReadings:\n\nChapters 1, 2 and 5. Cattaneo, Matias D., Nicolás Idrobo, and Rocío Titiunik. A practical introduction to regression discontinuity designs: Foundations. Cambridge University Press, 2019.\nEggers, A. C., Freier, R., Grembi, V., & Nannicini, T. (2018). Regression discontinuity designs based on population thresholds: Pitfalls and solutions. American Journal of Political Science, 62(1), 210-229.\nKeele, Luke J., and Rocio Titiunik. “Geographic boundaries as regression discontinuities.” Political Analysis 23.1 (2015): 127-155.\n\nApplications:\n\nHidalgo, F. Daniel, and Simeon Nichter. “Voter buying: Shaping the electorate through clientelism.” American Journal of Political Science 60.2 (2016): 436-455.\nBleemer, Zachary, and Aashish Mehta. “Will studying economics make you rich? A regression discontinuity analysis of the returns to college major.” American Economic Journal: Applied Economics 14.2 (2022): 1-22."
  },
  {
    "objectID": "schedule.html#week-13-april-20-24-mediation-and-sensitivity-analysis",
    "href": "schedule.html#week-13-april-20-24-mediation-and-sensitivity-analysis",
    "title": "Schedule",
    "section": "Week 13: April 20-24 | Mediation and Sensitivity Analysis",
    "text": "Week 13: April 20-24 | Mediation and Sensitivity Analysis\nMonday, April 20 & Wednesday, April 22\nTopics:\n\nHow to define and identify indirect and direct effects of treatment\nHow to assess the robustness of results to violations of identification assumptions\n\nReadings:\n\nBlackwell, Matthew. “A selection bias approach to sensitivity analysis for causal effects.” Political Analysis 22.2 (2014): 169-182.\nCinelli, Carlos, and Chad Hazlett. “Making sense of sensitivity: Extending omitted variable bias.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.1 (2020): 39-67.\nImai, K., Keele, L., Tingley, D., & Yamamoto, T. (2011). Unpacking the black box of causality: Learning about causal mechanisms from experimental and observational studies. American Political Science Review, 105(4), 765-789.\n\nBonus: Green, Donald P., Shang E. Ha, and John G. Bullock. “Enough already about”black box” experiments: Studying mediation is more difficult than most scholars suppose.” The Annals of the American Academy of Political and Social Science 628.1 (2010): 200-208.\n\nAcharya, Avidit, Matthew Blackwell, and Maya Sen. “Explaining causal findings without bias: Detecting and assessing direct effects.” American Political Science Review 110.3 (2016): 512-529."
  },
  {
    "objectID": "schedule.html#week-14-april-27---may-1-shift-share-designs-and-experiments-with-interference",
    "href": "schedule.html#week-14-april-27---may-1-shift-share-designs-and-experiments-with-interference",
    "title": "Schedule",
    "section": "Week 14: April 27 - May 1 | Shift-share designs and experiments with interference",
    "text": "Week 14: April 27 - May 1 | Shift-share designs and experiments with interference\nMonday, April 27 & Wednesday, April 29\nTopics:\n\nTBA\n\nReadings:\n\nTBA"
  },
  {
    "objectID": "supplementary_readings.html",
    "href": "supplementary_readings.html",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "This document contains recent methodology papers and political science applications that supplement the main course readings. For methodology, papers from economics, statistics, and sociology are included. For applications, the focus is on political science research.\n\n\n\n\n\n\nBai, Yuehao, Azeem M. Shaikh, and Max Tabord-Meehan. “A Primer on the Analysis of Randomized Experiments and a Survey of Some Recent Advances.” Journal of Political Economy Microeconomics (2024). [Revised April 2025]\n\nComprehensive survey covering randomization inference, covariate adjustment, stratification, and recent advances in experimental analysis.\n\n\n\n\n\n\nThe Lundberg, Johnson, and Stewart (2021) “What is your estimand?” paper already in your syllabus remains the key recent reference for this topic in social science.\n\n\n\n\n\n\n\n\n\nBroockman, David E., Elizabeth Rhodes, Alexander W. Bartik, et al. “The Causal Effects of Income on Political Attitudes and Behavior: A Randomized Field Experiment.” NBER Working Paper 33214 (2024).\n\nNovel field experiment providing experimental evidence on income effects.\n\nAthey, Susan, and Guido W. Imbens. Methods for analyzing heterogeneous treatment effects and machine learning for causal inference. Multiple papers 2023-2024.\n\nAdvances in estimating treatment effect heterogeneity using random forests and other ML methods.\n\n\n\n\n\n\nFukumoto, Kentaro. “What if neither randomized control trials nor public voting records are available in a get-out-the-vote field experiment?” Research & Politics (2023).\n\nInnovative GOTV study in Japan using RDD when RCT not possible.\n\nTesting theories of political persuasion using AI. PNAS (2024).\n\nUses LLMs to test persuasion theories in field experiments on immigration and education.\n\n\n\n\n\n\n\n\n\n\nLin, Winston. “Agnostic notes on regression adjustments to experimental data: Reexamining Freedman’s critique.” Annals of Applied Statistics 7.1 (2013): 295-318. [Already in syllabus - foundational]\nFDA Guidance. “Adjusting for Covariates in Randomized Clinical Trials for Drugs and Biological Products.” (2023).\n\nOfficial regulatory guidance on covariate adjustment methods.\n\nWang, Bingkai, et al. “When does adjusting covariate under randomization help? A comparative study on current practices.” BMC Medical Research Methodology (2024).\n\nCompares ANOVA, ANCOVA, ANHECOVA, IPW, AIPW, and overlap weighting estimators.\n\nSu, Fangzhou, and Peng Ding. Regression adjustment for cluster randomized experiments. (2021); Wang et al. (2024) extensions.\n\nModern approaches to cluster-randomized trial analysis.\n\n\n\n\n\n\nSee applications from Week 2; cluster-randomized trials are common in voter mobilization studies.\n\n\n\n\n\n\n\n\n\nGhanem, Dalia, et al. “Testing for Attrition Bias in Randomized Experiments.” (2023).\n\nDevelops formal tests for attrition bias in experiments.\n\nBai, Yuehao, et al. “Attrition in Matched Pair Experiments.” (2024).\n\nRevisits common approaches to attrition in matched-pair designs.\n\nPark, Chanwool, and Hyunseung Kang. “Causal Inference with Noncompliance and Unknown Interference.” Journal of the American Statistical Association (2023).\n\nHandles non-compliance when network interference is unknown.\n\nBugni, Federico, and Wayne Gao. (2023); Ren (2023); Bai et al. (2023) - Multiple papers on LATE estimation under non-compliance.\nChoi, David. “Profiling compliers and noncompliers for instrumental variable analysis with covariates: A weighting approach.” (2023).\n\n\n\n\n\nGeneralizing causal effects with noncompliance: Application to deep canvassing experiments. arXiv (2025).\n\nApplies IV approach to generalize from compliers to target populations.\n\n\n\n\n\n\n\n\n\n\nRosenman, Evan, et al. “Methods for Combining Observational and Experimental Causal Estimates: A Review.” WIREs Computational Statistics (2025).\n\nReviews approaches for integrating RCT and observational data.\n\nYang, Shu, et al. “Integrative estimator” for pooling RCT and observational data. (2023).\n\nNovel test for violations of unconfoundedness and transportability.\n\nDíaz, Iván, et al. “The Network Propensity Score.” arXiv (2022-2023).\n\nExtends propensity scores to settings with spillovers, homophily, and network formation.\n\nKing, Gary, and Richard Nielsen. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis (2019). [Important critique]\nMatchingFrontier Package. “Examining the Propensity Score Matching Paradox.” (2024).\n\nDemonstrates conditions under which PSM can increase bias.\n\n\n\n\n\n\nStudies in Political Science Research and Methods (August 2023) and Social Science Quarterly (January 2024) applying matching methods.\n“Schooling and Political Activism in the Early Civil Rights Era.” (January 2024).\n\nUses propensity score methods to study education effects on participation.\n\n\n\n\n\n\n\n\n\n\nLal, Apoorva, Mackenzie William Lockhart, Yiqing Xu, and Ziwen Zu. “How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on 67 Replicated Studies.” Political Analysis (2024).\n\nHighly recommended: Comprehensive replication study finding that IV strength is often overestimated and uncertainty underestimated.\n\nCinelli, Carlos, and Chad Hazlett. “An Omitted Variable Bias Framework for Sensitivity Analysis of Instrumental Variables.” Biometrika (2024/2025).\n\nExtends OVB sensitivity framework to IV designs.\n\nMellon, Jonathan. “Weather Instruments” critique (2023).\n\nExamines vulnerability of weather-based instruments.\n\n\n\n\n\n\nThe Lal et al. (2024) paper provides extensive discussion of IV applications across political science, including voting, representation, and conflict studies.\n\n\n\n\n\n\n\n\n\nRoth, Jonathan, Pedro H.C. Sant’Anna, Alyssa Bilinski, and John Poe. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics (2023). [Already in syllabus - essential]\nBaker, Andrew C., David F. Larcker, and Charles C.Y. Wang. “How Much Should We Trust Staggered Difference-in-Differences Estimates?” Journal of Financial Economics (2022).\nWooldridge, Jeffrey M. “Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators.” (2021, updated 2023).\n\nHeterogeneity-robust TWFE regressions; implemented in wooldid Stata package.\n\nCallaway, Brantly, and Pedro H.C. Sant’Anna. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics (2021).\n\nGroup-time ATT estimators; did R package.\n\nSun, Liyang, and Sarah Abraham. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics (2021).\nBorusyak, Kirill, Xavier Jaravel, and Jann Spiess. “Revisiting Event Study Designs: Robust and Efficient Estimation.” Review of Economic Studies (2024).\n\nImputation-based approach to staggered DiD.\n\nLiu, Licheng, Ye Wang, and Yiqing Xu. “A Practical Guide to Counterfactual Estimators for Causal Inference with Time-Series Cross-Sectional Data.” American Journal of Political Science (2024).\nArkhangelsky, Dmitry, and Guido W. Imbens. “Causal Models for Longitudinal and Panel Data: A Survey.” arXiv (2023).\n\nComprehensive survey of panel data methods including synthetic control.\n\n“Difference-in-Differences Meets Synthetic Control: Doubly Robust Identification and Estimation.” arXiv (2025).\n\nIntegrates DiD and synthetic control advantages.\n\n\n\n\n\n\nHassell, Hans J. G., and John B. Holbein. “Navigating Potential Pitfalls in Difference-in-Differences Designs: Reconciling Conflicting Findings on Mass Shootings’ Effect on Electoral Outcomes.” American Political Science Review (2024).\nLoeffler. “Does a Universal Basic Income Affect Voter Turnout? Evidence from Alaska.” Political Science Research and Methods (2023).\n\n\n\n\n\n\n\n\n\nCattaneo, Matias D., Nicolás Idrobo, and Rocío Titiunik. A Practical Introduction to Regression Discontinuity Designs: Extensions. Cambridge University Press (2024).\n\nHighly recommended: Second volume covering fuzzy RD, discrete scores, multi-dimensional RD, and local randomization framework.\n\nCattaneo, Idrobo, and Titiunik. RD Packages website: rdpackages.github.io\n\nSoftware for rdrobust, rdhte, rdlocrand, rddensity, rdpower.\n\nAuerbach, Eric. “Regression Discontinuity Design with Spillovers.” arXiv (2024).\n\nExtends RDD to settings with interference/spillovers.\n\n“Dynamic Regression Discontinuity under Treatment Effect Heterogeneity.” Quantitative Economics (2024).\n\nExtends RD to dynamic settings with repeated eligibility.\n\n“Hierarchical Regression Discontinuity Design: Pursuing Subgroup Treatment Effects.” arXiv (2023).\n\nHierarchical Bayes approach for subgroup analysis in RDD.\n\n\n\n\n\n\nMarshall, John. “Can Close Election Regression Discontinuity Designs Identify Effects of Winning Politician Characteristics?” American Journal of Political Science (2024).\n\nHighly recommended: Critical examination of politician characteristic RD designs.\n\nTorres, Santiago. “Close Elections Regression Discontinuity Designs in Multi-seat Systems.” (2023).\n\nExtends RDD beyond single-member districts.\n\nArchambault, Jerome, and Stanley L. Winer. “Political Competitiveness, Regression Discontinuity and the Incumbency Effect.” (2023).\n“The Role of Majority Status in Close Election Studies.” Political Analysis 32(1) (2024).\n\nDisentangles partisan effects from majority status effects in RD designs.\n\nHanretty, Chris. “How not to conduct a regression discontinuity design using a continuous measure of democracy.” Party Politics (2024).\n\nMethodological critique of misapplied RDD.\n\n\n\n\n\n\n\n\n\n\nPractical causal mediation analysis: extending nonparametric estimators to accommodate multiple mediators and multiple intermediate confounders. PMC (2024).\n\nHandles multiple mediators and post-exposure confounders.\n\n“Causal Mediation Analysis for Integrating Exposure, Genomic, and Phenotype Data.” PMC (2024).\n\nHigh-dimensional mediation analysis methods.\n\nKumar. “Poor economics and its missing mechanisms: The case for causal mediation.” Review of Development Economics (2024).\n\n\n\n\n\nCinelli, Carlos, and Chad Hazlett. “Making sense of sensitivity: Extending omitted variable bias.” JRSS-B (2020). [Already in syllabus - foundational]\nCinelli, Carlos, and Chad Hazlett. “An Omitted Variable Bias Framework for Sensitivity Analysis of Instrumental Variables.” Biometrika (2024/2025).\n\nExtends OVB framework to IV settings.\n\nMasten, Matthew A., Alexandre Poirier, and Linqi Zhang. “Assessing Sensitivity to Unconfoundedness: Estimation and Inference.” Journal of Business & Economic Statistics (January 2024).\n“Long Story Short: Omitted Variable Bias in Causal Machine Learning.” arXiv (updated May 2024).\n\nExtends sensitivity analysis to ML methods.\n\n\n\n\n\n\n“Comparative Causal Mediation and Relaxing the Assumption of No Mediator–Outcome Confounding: An Application to International Law and Audience Costs.” Political Analysis (2024).\n\nNovel comparative causal mediation estimands for multiple treatments.\n\nFabbe, Hazlett, and Sinmazdemir. “Threat perceptions, loyalties and attitudes towards peace: The effects of civilian victimization among Syrian refugees in Turkey.” Conflict Management and Peace Science (2024).\n\nApplied sensitivity analysis in conflict studies.\n\n\n\n\n\n\n\n\n\n\nBorusyak, Kirill, Peter Hull, and Xavier Jaravel. “Quasi-Experimental Shift-Share Research Designs.” Review of Economic Studies 89(1) (2022): 181-213.\n\nEssential: Shock-based identification framework for shift-share IV.\n\nGoldsmith-Pinkham, Paul, Isaac Sorkin, and Henry Swift. “Bartik Instruments: What, When, Why, and How.” American Economic Review 110(8) (2020): 2586-2624.\n\nEssential: Shares-based identification framework.\n\nBorusyak, Kirill, Peter Hull, and Xavier Jaravel. “A Practical Guide to Shift-Share Instruments.” NBER Working Paper 33236 (2024/2025).\n\nHighly recommended: Updated practical guidance synthesizing recent developments.\n\n\n\n\n\n\nSävje, Fredrik. “Causal inference with misspecified exposure mappings: separating definitions and assumptions.” Biometrika (2024).\nEgami, Naoki. “Identification of causal diffusion effects using placebo outcomes under structural stationarity.” Journal of the Royal Statistical Society (2024).\nChao, Spiegelman, Buchanan, and Forastiere. “Estimation and inference for causal spillover effects in egocentric-network randomized trials in the presence of network membership misclassification.” Biostatistics (2024).\nLeung, Michael P. (2022); Viviano et al. (2023) - Papers on approximate neighborhood interference (ANI).\n“Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference.” arXiv (2023).\n\nSynthetic control with network interference.\n\n\n\n\n\n\nSinclair, Betsy, et al. “Detecting Spillover Effects: Design and Analysis of Multilevel Experiments.” American Journal of Political Science (2012). [Foundational]\n“Spillover Effects in Experimental Data.” Chapter 16 in Advances in Experimental Political Science (Cambridge, 2021).\n“Spillover Effects in the Presence of Unobserved Networks.” Political Analysis.\nFyfe and Desmarais. “Causal Evidence for Theories of Contagious Civil Unrest.” International Studies Quarterly (2024).\nCruces, Tortarolo, and Vazquez-Bare. “Design of Partial Population Experiments with an Application to Spillovers in Tax Compliance.” IZA Discussion Paper (2024).\n\n\n\n\n\n\n\n\n\nCunningham, Scott. Causal Inference: The Mixtape. Yale University Press (2021). [Free online]\nHuntington-Klein, Nick. The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC (2021). [Free online]\nAngrist, Joshua D., and Jörn-Steffen Pischke. Mastering ’Metrics: The Path from Cause to Effect. Princeton University Press (2014).\n\n\n\n\n\nRD Packages: rdpackages.github.io\ndid Package (R): Callaway-Sant’Anna estimators\nwooldid (Stata): Wooldridge’s heterogeneity-robust TWFE\nsensemakr (R): Cinelli-Hazlett sensitivity analysis\n\n\nLast updated: January 2026"
  },
  {
    "objectID": "supplementary_readings.html#week-1-statistical-review-estimands",
    "href": "supplementary_readings.html#week-1-statistical-review-estimands",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Bai, Yuehao, Azeem M. Shaikh, and Max Tabord-Meehan. “A Primer on the Analysis of Randomized Experiments and a Survey of Some Recent Advances.” Journal of Political Economy Microeconomics (2024). [Revised April 2025]\n\nComprehensive survey covering randomization inference, covariate adjustment, stratification, and recent advances in experimental analysis.\n\n\n\n\n\n\nThe Lundberg, Johnson, and Stewart (2021) “What is your estimand?” paper already in your syllabus remains the key recent reference for this topic in social science."
  },
  {
    "objectID": "supplementary_readings.html#week-2-potential-outcomes-randomized-experiments",
    "href": "supplementary_readings.html#week-2-potential-outcomes-randomized-experiments",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Broockman, David E., Elizabeth Rhodes, Alexander W. Bartik, et al. “The Causal Effects of Income on Political Attitudes and Behavior: A Randomized Field Experiment.” NBER Working Paper 33214 (2024).\n\nNovel field experiment providing experimental evidence on income effects.\n\nAthey, Susan, and Guido W. Imbens. Methods for analyzing heterogeneous treatment effects and machine learning for causal inference. Multiple papers 2023-2024.\n\nAdvances in estimating treatment effect heterogeneity using random forests and other ML methods.\n\n\n\n\n\n\nFukumoto, Kentaro. “What if neither randomized control trials nor public voting records are available in a get-out-the-vote field experiment?” Research & Politics (2023).\n\nInnovative GOTV study in Japan using RDD when RCT not possible.\n\nTesting theories of political persuasion using AI. PNAS (2024).\n\nUses LLMs to test persuasion theories in field experiments on immigration and education."
  },
  {
    "objectID": "supplementary_readings.html#week-3-experiments---covariates-and-statistical-power",
    "href": "supplementary_readings.html#week-3-experiments---covariates-and-statistical-power",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Lin, Winston. “Agnostic notes on regression adjustments to experimental data: Reexamining Freedman’s critique.” Annals of Applied Statistics 7.1 (2013): 295-318. [Already in syllabus - foundational]\nFDA Guidance. “Adjusting for Covariates in Randomized Clinical Trials for Drugs and Biological Products.” (2023).\n\nOfficial regulatory guidance on covariate adjustment methods.\n\nWang, Bingkai, et al. “When does adjusting covariate under randomization help? A comparative study on current practices.” BMC Medical Research Methodology (2024).\n\nCompares ANOVA, ANCOVA, ANHECOVA, IPW, AIPW, and overlap weighting estimators.\n\nSu, Fangzhou, and Peng Ding. Regression adjustment for cluster randomized experiments. (2021); Wang et al. (2024) extensions.\n\nModern approaches to cluster-randomized trial analysis.\n\n\n\n\n\n\nSee applications from Week 2; cluster-randomized trials are common in voter mobilization studies."
  },
  {
    "objectID": "supplementary_readings.html#week-4-experiments---attrition-and-non-compliance",
    "href": "supplementary_readings.html#week-4-experiments---attrition-and-non-compliance",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Ghanem, Dalia, et al. “Testing for Attrition Bias in Randomized Experiments.” (2023).\n\nDevelops formal tests for attrition bias in experiments.\n\nBai, Yuehao, et al. “Attrition in Matched Pair Experiments.” (2024).\n\nRevisits common approaches to attrition in matched-pair designs.\n\nPark, Chanwool, and Hyunseung Kang. “Causal Inference with Noncompliance and Unknown Interference.” Journal of the American Statistical Association (2023).\n\nHandles non-compliance when network interference is unknown.\n\nBugni, Federico, and Wayne Gao. (2023); Ren (2023); Bai et al. (2023) - Multiple papers on LATE estimation under non-compliance.\nChoi, David. “Profiling compliers and noncompliers for instrumental variable analysis with covariates: A weighting approach.” (2023).\n\n\n\n\n\nGeneralizing causal effects with noncompliance: Application to deep canvassing experiments. arXiv (2025).\n\nApplies IV approach to generalize from compliers to target populations."
  },
  {
    "objectID": "supplementary_readings.html#weeks-5-7-selection-on-observables-propensity-scores-matching",
    "href": "supplementary_readings.html#weeks-5-7-selection-on-observables-propensity-scores-matching",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Rosenman, Evan, et al. “Methods for Combining Observational and Experimental Causal Estimates: A Review.” WIREs Computational Statistics (2025).\n\nReviews approaches for integrating RCT and observational data.\n\nYang, Shu, et al. “Integrative estimator” for pooling RCT and observational data. (2023).\n\nNovel test for violations of unconfoundedness and transportability.\n\nDíaz, Iván, et al. “The Network Propensity Score.” arXiv (2022-2023).\n\nExtends propensity scores to settings with spillovers, homophily, and network formation.\n\nKing, Gary, and Richard Nielsen. “Why Propensity Scores Should Not Be Used for Matching.” Political Analysis (2019). [Important critique]\nMatchingFrontier Package. “Examining the Propensity Score Matching Paradox.” (2024).\n\nDemonstrates conditions under which PSM can increase bias.\n\n\n\n\n\n\nStudies in Political Science Research and Methods (August 2023) and Social Science Quarterly (January 2024) applying matching methods.\n“Schooling and Political Activism in the Early Civil Rights Era.” (January 2024).\n\nUses propensity score methods to study education effects on participation."
  },
  {
    "objectID": "supplementary_readings.html#week-8-instrumental-variables",
    "href": "supplementary_readings.html#week-8-instrumental-variables",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Lal, Apoorva, Mackenzie William Lockhart, Yiqing Xu, and Ziwen Zu. “How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on 67 Replicated Studies.” Political Analysis (2024).\n\nHighly recommended: Comprehensive replication study finding that IV strength is often overestimated and uncertainty underestimated.\n\nCinelli, Carlos, and Chad Hazlett. “An Omitted Variable Bias Framework for Sensitivity Analysis of Instrumental Variables.” Biometrika (2024/2025).\n\nExtends OVB sensitivity framework to IV designs.\n\nMellon, Jonathan. “Weather Instruments” critique (2023).\n\nExamines vulnerability of weather-based instruments.\n\n\n\n\n\n\nThe Lal et al. (2024) paper provides extensive discussion of IV applications across political science, including voting, representation, and conflict studies."
  },
  {
    "objectID": "supplementary_readings.html#weeks-9-11-differences-in-differences-and-panel-data",
    "href": "supplementary_readings.html#weeks-9-11-differences-in-differences-and-panel-data",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Roth, Jonathan, Pedro H.C. Sant’Anna, Alyssa Bilinski, and John Poe. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics (2023). [Already in syllabus - essential]\nBaker, Andrew C., David F. Larcker, and Charles C.Y. Wang. “How Much Should We Trust Staggered Difference-in-Differences Estimates?” Journal of Financial Economics (2022).\nWooldridge, Jeffrey M. “Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators.” (2021, updated 2023).\n\nHeterogeneity-robust TWFE regressions; implemented in wooldid Stata package.\n\nCallaway, Brantly, and Pedro H.C. Sant’Anna. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics (2021).\n\nGroup-time ATT estimators; did R package.\n\nSun, Liyang, and Sarah Abraham. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics (2021).\nBorusyak, Kirill, Xavier Jaravel, and Jann Spiess. “Revisiting Event Study Designs: Robust and Efficient Estimation.” Review of Economic Studies (2024).\n\nImputation-based approach to staggered DiD.\n\nLiu, Licheng, Ye Wang, and Yiqing Xu. “A Practical Guide to Counterfactual Estimators for Causal Inference with Time-Series Cross-Sectional Data.” American Journal of Political Science (2024).\nArkhangelsky, Dmitry, and Guido W. Imbens. “Causal Models for Longitudinal and Panel Data: A Survey.” arXiv (2023).\n\nComprehensive survey of panel data methods including synthetic control.\n\n“Difference-in-Differences Meets Synthetic Control: Doubly Robust Identification and Estimation.” arXiv (2025).\n\nIntegrates DiD and synthetic control advantages.\n\n\n\n\n\n\nHassell, Hans J. G., and John B. Holbein. “Navigating Potential Pitfalls in Difference-in-Differences Designs: Reconciling Conflicting Findings on Mass Shootings’ Effect on Electoral Outcomes.” American Political Science Review (2024).\nLoeffler. “Does a Universal Basic Income Affect Voter Turnout? Evidence from Alaska.” Political Science Research and Methods (2023)."
  },
  {
    "objectID": "supplementary_readings.html#week-12-regression-discontinuity-designs",
    "href": "supplementary_readings.html#week-12-regression-discontinuity-designs",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Cattaneo, Matias D., Nicolás Idrobo, and Rocío Titiunik. A Practical Introduction to Regression Discontinuity Designs: Extensions. Cambridge University Press (2024).\n\nHighly recommended: Second volume covering fuzzy RD, discrete scores, multi-dimensional RD, and local randomization framework.\n\nCattaneo, Idrobo, and Titiunik. RD Packages website: rdpackages.github.io\n\nSoftware for rdrobust, rdhte, rdlocrand, rddensity, rdpower.\n\nAuerbach, Eric. “Regression Discontinuity Design with Spillovers.” arXiv (2024).\n\nExtends RDD to settings with interference/spillovers.\n\n“Dynamic Regression Discontinuity under Treatment Effect Heterogeneity.” Quantitative Economics (2024).\n\nExtends RD to dynamic settings with repeated eligibility.\n\n“Hierarchical Regression Discontinuity Design: Pursuing Subgroup Treatment Effects.” arXiv (2023).\n\nHierarchical Bayes approach for subgroup analysis in RDD.\n\n\n\n\n\n\nMarshall, John. “Can Close Election Regression Discontinuity Designs Identify Effects of Winning Politician Characteristics?” American Journal of Political Science (2024).\n\nHighly recommended: Critical examination of politician characteristic RD designs.\n\nTorres, Santiago. “Close Elections Regression Discontinuity Designs in Multi-seat Systems.” (2023).\n\nExtends RDD beyond single-member districts.\n\nArchambault, Jerome, and Stanley L. Winer. “Political Competitiveness, Regression Discontinuity and the Incumbency Effect.” (2023).\n“The Role of Majority Status in Close Election Studies.” Political Analysis 32(1) (2024).\n\nDisentangles partisan effects from majority status effects in RD designs.\n\nHanretty, Chris. “How not to conduct a regression discontinuity design using a continuous measure of democracy.” Party Politics (2024).\n\nMethodological critique of misapplied RDD."
  },
  {
    "objectID": "supplementary_readings.html#week-13-mediation-and-sensitivity-analysis",
    "href": "supplementary_readings.html#week-13-mediation-and-sensitivity-analysis",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Practical causal mediation analysis: extending nonparametric estimators to accommodate multiple mediators and multiple intermediate confounders. PMC (2024).\n\nHandles multiple mediators and post-exposure confounders.\n\n“Causal Mediation Analysis for Integrating Exposure, Genomic, and Phenotype Data.” PMC (2024).\n\nHigh-dimensional mediation analysis methods.\n\nKumar. “Poor economics and its missing mechanisms: The case for causal mediation.” Review of Development Economics (2024).\n\n\n\n\n\nCinelli, Carlos, and Chad Hazlett. “Making sense of sensitivity: Extending omitted variable bias.” JRSS-B (2020). [Already in syllabus - foundational]\nCinelli, Carlos, and Chad Hazlett. “An Omitted Variable Bias Framework for Sensitivity Analysis of Instrumental Variables.” Biometrika (2024/2025).\n\nExtends OVB framework to IV settings.\n\nMasten, Matthew A., Alexandre Poirier, and Linqi Zhang. “Assessing Sensitivity to Unconfoundedness: Estimation and Inference.” Journal of Business & Economic Statistics (January 2024).\n“Long Story Short: Omitted Variable Bias in Causal Machine Learning.” arXiv (updated May 2024).\n\nExtends sensitivity analysis to ML methods.\n\n\n\n\n\n\n“Comparative Causal Mediation and Relaxing the Assumption of No Mediator–Outcome Confounding: An Application to International Law and Audience Costs.” Political Analysis (2024).\n\nNovel comparative causal mediation estimands for multiple treatments.\n\nFabbe, Hazlett, and Sinmazdemir. “Threat perceptions, loyalties and attitudes towards peace: The effects of civilian victimization among Syrian refugees in Turkey.” Conflict Management and Peace Science (2024).\n\nApplied sensitivity analysis in conflict studies."
  },
  {
    "objectID": "supplementary_readings.html#week-14-shift-share-designs-and-experiments-with-interference",
    "href": "supplementary_readings.html#week-14-shift-share-designs-and-experiments-with-interference",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Borusyak, Kirill, Peter Hull, and Xavier Jaravel. “Quasi-Experimental Shift-Share Research Designs.” Review of Economic Studies 89(1) (2022): 181-213.\n\nEssential: Shock-based identification framework for shift-share IV.\n\nGoldsmith-Pinkham, Paul, Isaac Sorkin, and Henry Swift. “Bartik Instruments: What, When, Why, and How.” American Economic Review 110(8) (2020): 2586-2624.\n\nEssential: Shares-based identification framework.\n\nBorusyak, Kirill, Peter Hull, and Xavier Jaravel. “A Practical Guide to Shift-Share Instruments.” NBER Working Paper 33236 (2024/2025).\n\nHighly recommended: Updated practical guidance synthesizing recent developments.\n\n\n\n\n\n\nSävje, Fredrik. “Causal inference with misspecified exposure mappings: separating definitions and assumptions.” Biometrika (2024).\nEgami, Naoki. “Identification of causal diffusion effects using placebo outcomes under structural stationarity.” Journal of the Royal Statistical Society (2024).\nChao, Spiegelman, Buchanan, and Forastiere. “Estimation and inference for causal spillover effects in egocentric-network randomized trials in the presence of network membership misclassification.” Biostatistics (2024).\nLeung, Michael P. (2022); Viviano et al. (2023) - Papers on approximate neighborhood interference (ANI).\n“Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference.” arXiv (2023).\n\nSynthetic control with network interference.\n\n\n\n\n\n\nSinclair, Betsy, et al. “Detecting Spillover Effects: Design and Analysis of Multilevel Experiments.” American Journal of Political Science (2012). [Foundational]\n“Spillover Effects in Experimental Data.” Chapter 16 in Advances in Experimental Political Science (Cambridge, 2021).\n“Spillover Effects in the Presence of Unobserved Networks.” Political Analysis.\nFyfe and Desmarais. “Causal Evidence for Theories of Contagious Civil Unrest.” International Studies Quarterly (2024).\nCruces, Tortarolo, and Vazquez-Bare. “Design of Partial Population Experiments with an Application to Spillovers in Tax Compliance.” IZA Discussion Paper (2024)."
  },
  {
    "objectID": "supplementary_readings.html#additional-cross-cutting-resources",
    "href": "supplementary_readings.html#additional-cross-cutting-resources",
    "title": "Supplementary Readings for PS 813: Causal Inference",
    "section": "",
    "text": "Cunningham, Scott. Causal Inference: The Mixtape. Yale University Press (2021). [Free online]\nHuntington-Klein, Nick. The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC (2021). [Free online]\nAngrist, Joshua D., and Jörn-Steffen Pischke. Mastering ’Metrics: The Path from Cause to Effect. Princeton University Press (2014).\n\n\n\n\n\nRD Packages: rdpackages.github.io\ndid Package (R): Callaway-Sant’Anna estimators\nwooldid (Stata): Wooldridge’s heterogeneity-robust TWFE\nsensemakr (R): Cinelli-Hazlett sensitivity analysis\n\n\nLast updated: January 2026"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PoliSci 813",
    "section": "",
    "text": "Causal Inference\nDesigning studies to uncover cause and effect from data\nPoliSci 813 • Spring 2026University of Wisconsin-Madison\n\n\n\n\n\n\n\n\nInstructor\n\n   Anton Strezhnev\n   North Hall 322D\n   strezhnev@wisc.edu\n\n\n\nTeaching Assistant\n\n   Junda Li\n   jli2479@wisc.edu\n\n\n\n\nCourse details\n\n   Mondays & Wednesdays\n   1/21/2026 - 5/4/2026\n   9:30am - 10:45am\n   North Hall 422\n\n\n\nContacting me\nE-mail is the best way to get in touch with me. My dedicated office hours are Tuesdays from 2pm-4pm if you want to meet in-person. I am also generally in my office on weekdays during normal hours and my door is open, so you are welcome to drop in. You can also feel free to e-mail me to confirm availability or to schedule a virtual meeting via Zoom."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "These textbooks provide a number of the readings that will be used through the course of the semester. Each is available either as a free online version or in digital form via UW-Madison library access.\n\nCunningham, Scott. Causal inference: The Mixtape. Yale University Press, 2021.\n\nFree online version\n\nHuntington-Klein, Nick. The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC, 2021.\n\nFree online version\n\nHernán, Miguel A. and James M. Robins. Causal Inference: What If. Chapman & Hall/CRC. 2020.\n\nFree PDF available\n\nImbens, Guido W. and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press. 2015.\n\nUW-Madison Library Access\n\nMorgan, Stephen L., and Christopher Winship. Counterfactuals and Causal Inference. Cambridge University Press, 2015.\n\nUW-Madison Library Access\n\nWager, Stefan. Causal inference: A statistical learning approach.\n\nDraft PDF available here"
  },
  {
    "objectID": "resources.html#textbooks",
    "href": "resources.html#textbooks",
    "title": "Resources",
    "section": "",
    "text": "These textbooks provide a number of the readings that will be used through the course of the semester. Each is available either as a free online version or in digital form via UW-Madison library access.\n\nCunningham, Scott. Causal inference: The Mixtape. Yale University Press, 2021.\n\nFree online version\n\nHuntington-Klein, Nick. The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC, 2021.\n\nFree online version\n\nHernán, Miguel A. and James M. Robins. Causal Inference: What If. Chapman & Hall/CRC. 2020.\n\nFree PDF available\n\nImbens, Guido W. and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press. 2015.\n\nUW-Madison Library Access\n\nMorgan, Stephen L., and Christopher Winship. Counterfactuals and Causal Inference. Cambridge University Press, 2015.\n\nUW-Madison Library Access\n\nWager, Stefan. Causal inference: A statistical learning approach.\n\nDraft PDF available here"
  },
  {
    "objectID": "resources.html#software",
    "href": "resources.html#software",
    "title": "Resources",
    "section": "Software",
    "text": "Software\n\nR and RStudio\nThis course uses R for all programming assignments. You are welcome to use the RStudio Interactive Development Environment (IDE) to write code and edit the assignment write-up. Due to the extensive integration of Quarto into Posit-developed IDEs, I encourage using either RStudio or the more recent Positron IDE.\n\nR: Download from https://www.r-project.org\nRStudio: Download from https://posit.co/download/rstudio-desktop/\nPositron: Download from https://positron.posit.co/\n\n\nRecommended Guides for R Programming\n\nR For Data Science Available at https://r4ds.hadley.nz/\nData Visualization: A Practical Introduction Available at https://socviz.co/\n\n\n\n\nQuarto and Markdown\nThe assignments are distributed as .qmd Quarto files. Quarto is the successor to R Markdown and lets you present your analysis, code, figures and written discussion all in a single document. It uses Markdown syntax for formatting text and supports embedded R code chunks that execute when you render the document. This lets you present your analysis, code, figures, and written discussion all in one place.\nTo complete assignments, you will edit the provided .qmd file, adding your code and written responses, then render it to an HTML file for submission. Both your .html output and .qmd file will be submitted via Gradescope\n\nQuarto: Download from https://quarto.org/docs/get-started/\nQuarto Guide: https://quarto.org/docs/guide/\nMarkdown Basics: https://quarto.org/docs/authoring/markdown-basics.html\nNote also the chapter on Quarto in R For Data Science"
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Staff",
    "section": "",
    "text": "Assistant Professor\nDepartment of Political Science\nUniversity of Wisconsin-Madison\n\n\n\n\n\nOffice\nNorth Hall 322D\n\n\nEmail\nstrezhnev@wisc.edu\n\n\nWebsite\nantonstrezhnev.com\n\n\nOffice Hours\nTuesdays, 2pm-4pm (or by appointment via email)"
  },
  {
    "objectID": "staff.html#instructor",
    "href": "staff.html#instructor",
    "title": "Staff",
    "section": "",
    "text": "Assistant Professor\nDepartment of Political Science\nUniversity of Wisconsin-Madison\n\n\n\n\n\nOffice\nNorth Hall 322D\n\n\nEmail\nstrezhnev@wisc.edu\n\n\nWebsite\nantonstrezhnev.com\n\n\nOffice Hours\nTuesdays, 2pm-4pm (or by appointment via email)"
  },
  {
    "objectID": "staff.html#teaching-assistant",
    "href": "staff.html#teaching-assistant",
    "title": "Staff",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\n\nJunda Li\n\n\nPhD Candidate\nDepartment of Political Science\nUniversity of Wisconsin-Madison\n\n\n\n\n\nEmail\njli2458@wisc.edu\n\n\nWebsite\njundali.org\n\n\nOffice Hours\nTBA"
  },
  {
    "objectID": "staff.html#contact-policy",
    "href": "staff.html#contact-policy",
    "title": "Staff",
    "section": "Contact Policy",
    "text": "Contact Policy\n\nCourse questions: Please post questions about course content on the Ed discussion forum so that all students can benefit from the answers.\nPersonal matters: Email the instructor directly.\nProblem set questions: Post on the discussion forum or come to office hours.\nExpected response time: We aim to respond to discussion posts within 24 hours on weekdays."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Questions of cause and effect are central to the study of political science and to the social sciences more broadly. But making inferences about causation from empirical data is a significant challenge. Critically, there is no simple, assumption-free process for learning about a causal relationship from the data alone. Causal inference requires researchers to make assumptions about the underlying data generating process in order to identify and estimate causal effects. The goal of this course is to provide students with a structured statistical framework for articulating the assumptions behind causal research designs and for estimating effects using quantitative data.\nThe course begins by introducing the counterfactual framework of causal inference as a way of defining causal quantities of interest such as the “average treatment effect.” It then proceeds to illustrate a variety of different designs for identifying and estimating these quantities. We will start with the most basic experimental designs and progress to more complex experimental and observational methods. For each approach, we will discuss the necessary assumptions that a researcher needs to make about the process that generated the data, how to assess whether these assumptions are reasonable, how to interpret the quantity being estimated and ultimately how to conduct the analysis.\nThis course will involve a combination of lectures and problem sets. Problem sets will contain a mixture of both theoretical and applied questions and serve to reinforce key concepts and allow students to assess their progress and understanding throughout the course. Assignments will involve analysis of data using the  programming language. This is a free and open source language for statistical computing that is used extensively for data analysis in many fields. Prior experience with the fundamentals of  programming is required."
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "",
    "text": "Questions of cause and effect are central to the study of political science and to the social sciences more broadly. But making inferences about causation from empirical data is a significant challenge. Critically, there is no simple, assumption-free process for learning about a causal relationship from the data alone. Causal inference requires researchers to make assumptions about the underlying data generating process in order to identify and estimate causal effects. The goal of this course is to provide students with a structured statistical framework for articulating the assumptions behind causal research designs and for estimating effects using quantitative data.\nThe course begins by introducing the counterfactual framework of causal inference as a way of defining causal quantities of interest such as the “average treatment effect.” It then proceeds to illustrate a variety of different designs for identifying and estimating these quantities. We will start with the most basic experimental designs and progress to more complex experimental and observational methods. For each approach, we will discuss the necessary assumptions that a researcher needs to make about the process that generated the data, how to assess whether these assumptions are reasonable, how to interpret the quantity being estimated and ultimately how to conduct the analysis.\nThis course will involve a combination of lectures and problem sets. Problem sets will contain a mixture of both theoretical and applied questions and serve to reinforce key concepts and allow students to assess their progress and understanding throughout the course. Assignments will involve analysis of data using the  programming language. This is a free and open source language for statistical computing that is used extensively for data analysis in many fields. Prior experience with the fundamentals of  programming is required."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course is the second in the political science graduate methodology sequence. Completing the introductory course prior to this sequence should prepare you for the material in this class. We will rely on some background knowledge of core concepts in probability, statistics and inference as well as experience with statistical programming in . However, there are no strict, specific course pre-requisites as many different disciplines and departments offer introductory statistics classes that cover the relevant material.\nIn general, you should have had some introduction to probability theory and should be familiar with concepts like the properties of random variables (especially expectation and variance), estimands and estimators, and statistical inference. Familiarity with linear regression is also a plus, but we will be reviewing it during the relevant week.\nPlease contact the instructor if you are interested in enrolling but are unsure of the requirements."
  },
  {
    "objectID": "syllabus.html#logistics",
    "href": "syllabus.html#logistics",
    "title": "Syllabus",
    "section": "Logistics",
    "text": "Logistics\n\nLectures: Mondays/Wednesdays from 9:30am-10:45am\n\nYou should attend lectures regularly as they comprise a significant element of the course instruction. Lecture materials will be posted on the course website.\n\nDiscussion Forum: We will be using Ed as our primary course discussion platform. If you are enrolled in the class on Canvas, you should already have access to the Ed board for this class. Please use this to post questions about the readings/lecture material as well as about the problem sets.\nCourse Materials: Lecture materials, problem sets and tutorial code will be posted on the course website. Problem set solutions will be posted after the due date on Canvas. Links to readings can be found on the course website organized by week."
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThe course will involve readings from a variety of different textbooks and published papers. The class will not require the purchase of a single, specific, text and all excerpts from textbooks are available online (either directly or through library resources). However, you may wish to obtain some of these texts to use as a personal reference and they may be valuable to you in the future.\n\nRecommended Textbooks\n\nImbens, Guido W. and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press. 2010.\nHernán, Miguel A. and James M. Robins. Causal Inference: What If. Chapman & Hall/CRC. 2020.\nMorgan, Stephen L., and Christopher Winship. Counterfactuals and Causal Inference. Cambridge University Press, 2015.\nCunningham, Scott. Causal inference: The Mixtape. Yale University Press, 2021.\nHuntington-Klein, Nick. The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC, 2021.\nWager, Stefan. Causal inference: A statistical learning approach. Draft available here"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nStudents’ final grades are based on four components:\n\nProblem Sets (25%)\nStudents will complete a total of five problem sets throughout the quarter. Problem sets will cover roughly a two-week period of course material. A complete schedule of the assignments can be found on the Assignments page.\nThe goal of the problem sets is to encourage exploration of the material and to provide you with a clear and credible means of assessing your understanding and progress through the course. As such, problem sets are designed to be challenging and we expect students to find some questions difficult.\nProblem sets will be graded on a (+/✓/-) scale:\n\n+ : Complete and near-perfect work\n✓ : Generally good work with clear effort shown but with notable errors\n- : Significantly incomplete work with major conceptual errors and little effort shown\n\n\nCollaboration Policy\nWe strongly encourage collaboration between students on the problem sets and highly recommend that students discuss problems with each other either in person or via the discussion forum. However, each student is expected to submit their own write-up of the answers and any relevant code.\n\n\nOffice Hours and Online Discussion\nStudents should feel free to discuss any questions about the problem sets with the teaching staff during sections and office hours. We also strongly encourage students to post questions about both the problem sets and the assigned readings on the course discussion board and respond to other students’ questions. Responding to other students’ questions will contribute to your participation grade.\n\n\nSubmission Guidelines\nProblem sets will be distributed as HTMl and Quarto files (.qmd). You should submit your answers and any relevant R code in the same format: including the Quarto file (.qmd extension) and a corresponding rendered .html file as your submission. You will be submitting your problem sets via Gradescope.\n\n\n\nIn-person Midterm (30%)\nWe will have an in-person midterm examination on Wednesday, March 4th, 2026. This exam will cover the material in the first half of the course (experiments and selection-on-observables) The exam will take the form of a standard pen-and-paper timed examination involving both theory and practical analysis of sample code and results.\n\n\nIn-person Final (35%)\nThe final exam will take place in person during exams week. We have been assigned an exam date of May 4, 2026 at 5:05PM - 7:05PM. Room information will be made available nearer to the end of the semester. The exam will take the same form as the midterm, but will be slightly longer given the additional time available during the exam period.\n\n\nParticipation (10%)\nWe expect students to take an active role in learning in both lecture and section. Engagement with the teaching staff by asking and answering questions will contribute to this grade as will interaction on the discussion board."
  },
  {
    "objectID": "syllabus.html#computing",
    "href": "syllabus.html#computing",
    "title": "Syllabus",
    "section": "Computing",
    "text": "Computing\nThis course will use the R programming language. This is a free and open source programming language that is available for nearly all computing platforms. You should download and install it from https://www.r-project.org.\nUnless you have strong preferences for a specific coding environment, we recommend that you use the free RStudio Desktop Integrated Development Environment (IDE) which you can download from https://rstudio.com/products/rstudio/download/#download.\nIn addition to base R, we will be frequently using data management and processing tools found in the tidyverse set of packages along with basic graphics and visualization using ggplot2.\nSee the Resources page for additional information."
  },
  {
    "objectID": "syllabus.html#policy-on-generative-large-language-models",
    "href": "syllabus.html#policy-on-generative-large-language-models",
    "title": "Syllabus",
    "section": "Policy on Generative Large Language Models",
    "text": "Policy on Generative Large Language Models\nLarge Language Models (LLMs) continue to have an immense impact on the educational field. Over the last several years, we have seen striking growth in the capabilities of these models and it is clear that they will remain a permanent and inextricable presence in all of our lives. This very course website has been built from my earlier .tex syllabus with considerable assistance from Claude’s Opus 4.5 model. Having taught a causal inference course every year since 2020, I have seen first-hand how these tools are rapidly reshaping how students engage with the material - for better and for worse.\nThe clearest setting where LLM tools have been amplifying user productivity is in programming. For me, the most positive case for LLMs is that they are a kind of “universal interface” in natural language that makes it easier for users to get a computer to do what they want simply by articulating the task in English. Cutting-edge LLMs implement some form of “reasoning” (essentially, having the model generate intermediate tokens into its context window to improve the quality of the final predicted tokens) and models are able to call various software tools as part of the generation process. Modern LLMs are increasingly “agentic” in that they replicate the sort of think-decide-act loop that we would consider to be the role of a typical knowledge worker. In essence, every graduate student can now have their own team of RAs for a comparatively low monthly payment to Anthropic.\nUnfortunately, the effectiveness of LLMs remains highly variable across different task domains. My own view is that coding (especially coding for software development) is an optimal use case for LLMs because it is an engineering task. By “engineering,” I mean the the iterative task of solving a problem by brainstorming potential solutions, implementing those solutions, and then subsequently evaluating the solutions with respect to some clearly defined criteria. This is because the goal of engineering is the building of systems. The key components here are both the existence of a well-defined problem and the ability to assess whether the proposed solutions are effective. Indeed, modern agentic LLMs are good in large part because they incorporate a lot of this evaluation by way of the “reasoning” structure and have the ability to write unit tests to check the code. Moreover, software development is amenable to this “try-and-check” sort of reasoning because the sub-tasks typically involve some well-defined, structured output that can be evaluated against what the user knows what they need/want.\nProblems begin to accumulate when there is no longer an agent (human or LLM) that can diagnose errors and adjust. Unfortunately, this is the sort of thing that experienced users are better at than less experienced users. Without an understanding of what the LLM is actually doing, you are much less likely to be able to figure out when it has gone wrong. And the fail cases for LLMs are, in my experience, a lot weirder than the usual fail cases for humans. So my most negative assessment of LLMs is that they have severely adversely affected the process of education. While they may supercharge the productivity of those who already have the relevant understanding, they are much less promising in training novices and provide only the illusion of capability.\nI am increasingly opposed to the use of LLMs for programming when one is learning to code. In practice, I find that students delegate far too much to the model and spend insufficient time understanding the mechanics of what the code is doing. This makes it actually quite difficult to meaningfully debug outputs and understand how to diagnose errors. Additionally, actually implementing statistical methods in code is a common way for students to actually understand conceptually what the methods do and how they work. As such, I would still discourage LLM-assistance when completing problem sets although I do not strictly rule it out, especially for the more tedious components where you are confident that you understand what the code is doing (e.g. data cleaning).\nAdditionally, the sort of coding that we do as scientists is typically not software development, so I actually don’t think these tools are (yet) good for writing research code. The task of science I think is distinct from engineering in that we do not have a “try-check-fail-repeat” evaluation loop where the “correctness” of the method and implementation can be assessed from the “correctness” of the output. In fact, we would probably characterize scientists that do this sort of thing as engaging in “questionable research practices” if “correctness” is judged by the “number of publications” objective function. We are not building, we are discovering. A core theme of this class is the importance of outside information and deep knowledge in assessing the feasibility of the assumptions behind a particular research design. The quality of research is determined not by its outputs but rather by its inputs. These are not assumptions that an LLM (for the most part) will provide to you unprompted. An LLM agent can effectively replicate an existing analysis, but it is unlikely to tell you that this was the wrong analysis to begin with. I like to think of LLM agents as amateur improvisers - they can “Yes, and…” you very well, but are much less capable of the “No, but…”\nOne other common use of LLMs that I have also seen - and perhaps this is more common than direct assistance on assignments - is the use of them as “personal tutors.” The chat interfaces let students ask questions, summarize complex texts, search for other relevant information, and brainstorm (see the above point re: “amateur improviser”) without the need for awkward conversations with other human beings. While I think this is fine for simple questions, especially when the LLM just replaces web search, I would not use LLMs as a complete substitute for your colleagues and for the teaching staff. One of the unfortunate consequences of LLM-proliferation is that students don’t post on discussion boards as often as they used to - even in graduate classes. I feel that this is ultimately detrimental to the sort of community-building and professionalization that this class is designed for. Additionally, one of the benefits of asking the teaching staff is that they are familiar enough with the topic and the context that they can infer a lot of what is unstated or implied by your question and better tailor the response. We will provide additional context unprompted.\nWith regards to web-search, I would have strongly advised against this a few years ago, but the incorporation of retrieval augmented generation in modern models has reduced the prevalence of entirely made up sources, though not entirely. Since modern web search runs on basically the same underlying vector search architecture, there’s basically no difference in using an LLM vs. just Googling something. I would still encourage going to the retrieved sources directly though and avoiding the generated summaries since those seem to use cheaper/low-quality models. For summaries, tools explicitly built for interacting with a text (e.g. Google’s NotebookLM) I think are potentially more useful, though I have not spent much time exploring them. Even then, I would still encourage the “classical” approach of searching past (and, with Google Scholar, future) citations. Even if LLMs generated perfect summaries, I think there is an inherent benefit of the “old ways” when it comes to professionalization and developing a better understanding of the authors in a given literature. Unlike the rest of the internet, academia is still a link(citation)-based culture and it is worth leveraging that researcher-provided context to guide your reading. Additionally, these authors are real human beings who you will meet at conferences - it’s worth understanding who they are conversing with in order to better understand the discipline.\nLastly, any LLM policy needs to consider its feasibility. It is clear to me that any restrictions on LLM use aside from restricted, in-class pen-and-paper examination use are fundamentally unenforceable. Therefore, with respect to the problem sets, students are permitted to use LLMs in whatever capacity they see fit. I have attempted to design the problem sets such that they contain “out-of-distribution” challenges (e.g. a replication of an existing paper that concludes contrary to the original result) and otherwise general “traps” that try to evaluate deep substantive knowledge of the problem. Over the last two years, I have found on (e.g. take home exams) that LLM-using students produce mediocre but not completely terrible results. Nevertheless, they do make mistakes (and often behave in ways that could be described as “not wrong, just strange”) and it is clear to me which students use them to their detriment. Perhaps this will change in the next year or two - such is the nature of this field. Indeed, my decision to move entirely to in-person assessment was driven by the observation that although take-home exams still provided some variation among students, that variation was dramatically lower than the in-class exams. So in short:\n\nYou may use them.\nI discourage it for anything important.\nI’ll know if you use them poorly.\nIgnore the problem sets at your own peril. The consequences will arrive at exam time."
  },
  {
    "objectID": "syllabus.html#accommodations-and-accessibility",
    "href": "syllabus.html#accommodations-and-accessibility",
    "title": "Syllabus",
    "section": "Accommodations and Accessibility",
    "text": "Accommodations and Accessibility\nThe University of Wisconsin–Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute (36.12), and UW–Madison policy (Faculty Document 1071) require that students with disabilities be reasonably accommodated in instruction and campus life. Reasonable accommodations for students with disabilities is a shared faculty and student responsibility.\nStudents are expected to inform faculty of their need for instructional accommodations by the end of the third week of the semester, or as soon as possible after a disability has been incurred or recognized.\nI will work either directly with you or in coordination with the McBurney Disability Resource Center to identify and provide reasonable instructional accommodations. Once you are approved for accommodations by the McBurney Center, please be sure to make the relevant selections in McBurney Connect. When I have received your Student Accommodation Letter, I will send you a follow-up e-mail to connect and discuss how the accommodations will be implemented for this course. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA."
  },
  {
    "objectID": "syllabus.html#acknowledgments",
    "href": "syllabus.html#acknowledgments",
    "title": "Syllabus",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis course is indebted to the many wonderful and generous scholars who have developed causal inference curricula in political science departments throughout the world and who have made their course materials available to the public. In particular, I thank Matthew Blackwell, Brandon Stewart, Molly Roberts, Kosuke Imai, Teppei Yamamoto, Jens Hainmueller, Adam Glynn, Gary King, and Justin Grimmer whose lecture notes and syllabi have been immensely valuable in the creation of this course. Special thanks to Molly Offer-Westort, Andy Eggers and Bobby Gulotty who helped in the development of this course in its earlier incarnation as PLSC 30600 at the University of Chicago. I also thank the previous teaching assistants of this course: Arthur Yu, Oscar Cuadros, Zikai Li, and Cindy Wang.\nLastly, thanks to Andrew Heiss and Matt Blackwell for their Quarto website templates, which I have extensively borrowed from in designing this course site."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nAssigned\nDue\n\n\n\n\nProblem Set 1\nJan 21 (Week 1)\nFeb 4 (Week 3)\n\n\nProblem Set 2\nFeb 4 (Week 3)\nFeb 18 (Week 5)\n\n\nProblem Set 3\nFeb 18 (Week 5)\nMarch 4 (Week 7)\n\n\nMidterm Exam\nMarch 4 (Week 7)\nIn-Class\n\n\nProblem Set 4\nMarch 11 (Week 8)\nMarch 25 (Week 10)\n\n\nProblem Set 5\nApril 8 (Week 11)\nApril 22 (Week 13)\n\n\nFinal Exam\nMay 4 (Exam Week)\nLocation TBA"
  },
  {
    "objectID": "assignments.html#schedule",
    "href": "assignments.html#schedule",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nAssigned\nDue\n\n\n\n\nProblem Set 1\nJan 21 (Week 1)\nFeb 4 (Week 3)\n\n\nProblem Set 2\nFeb 4 (Week 3)\nFeb 18 (Week 5)\n\n\nProblem Set 3\nFeb 18 (Week 5)\nMarch 4 (Week 7)\n\n\nMidterm Exam\nMarch 4 (Week 7)\nIn-Class\n\n\nProblem Set 4\nMarch 11 (Week 8)\nMarch 25 (Week 10)\n\n\nProblem Set 5\nApril 8 (Week 11)\nApril 22 (Week 13)\n\n\nFinal Exam\nMay 4 (Exam Week)\nLocation TBA"
  },
  {
    "objectID": "assignments.html#problem-sets",
    "href": "assignments.html#problem-sets",
    "title": "Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\nProblem sets are distributed as .qmd files along with their rendered .html form. You should edit the provided .qmd file and render to .html when submitting the assignment. Additional files (e.g. datasets) are located in sub-folders. For convenience, all the combined files are also distributed as a .zip compressed archive.\n\nProblem Set 1\n\n[Github Repository]\n[.zip archive]"
  }
]