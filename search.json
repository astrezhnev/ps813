[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Questions of cause and effect are central to the study of political science and to the social sciences more broadly. But making inferences about causation from empirical data is a significant challenge. Critically, there is no simple, assumption-free process for learning about a causal relationship from the data alone. Causal inference requires researchers to make assumptions about the underlying data generating process in order to identify and estimate causal effects. The goal of this course is to provide students with a structured statistical framework for articulating the assumptions behind causal research designs and for estimating effects using quantitative data.\nThe course begins by introducing the counterfactual framework of causal inference as a way of defining causal quantities of interest such as the “average treatment effect.” It then proceeds to illustrate a variety of different designs for identifying and estimating these quantities. We will start with the most basic experimental designs and progress to more complex experimental and observational methods. For each approach, we will discuss the necessary assumptions that a researcher needs to make about the process that generated the data, how to assess whether these assumptions are reasonable, how to interpret the quantity being estimated and ultimately how to conduct the analysis.\nThis course will involve a combination of lectures and problem sets. Problem sets will contain a mixture of both theoretical and applied questions and serve to reinforce key concepts and allow students to assess their progress and understanding throughout the course. Assignments will involve analysis of data using the  programming language. This is a free and open source language for statistical computing that is used extensively for data analysis in many fields. Prior experience with the fundamentals of  programming is required."
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "",
    "text": "Questions of cause and effect are central to the study of political science and to the social sciences more broadly. But making inferences about causation from empirical data is a significant challenge. Critically, there is no simple, assumption-free process for learning about a causal relationship from the data alone. Causal inference requires researchers to make assumptions about the underlying data generating process in order to identify and estimate causal effects. The goal of this course is to provide students with a structured statistical framework for articulating the assumptions behind causal research designs and for estimating effects using quantitative data.\nThe course begins by introducing the counterfactual framework of causal inference as a way of defining causal quantities of interest such as the “average treatment effect.” It then proceeds to illustrate a variety of different designs for identifying and estimating these quantities. We will start with the most basic experimental designs and progress to more complex experimental and observational methods. For each approach, we will discuss the necessary assumptions that a researcher needs to make about the process that generated the data, how to assess whether these assumptions are reasonable, how to interpret the quantity being estimated and ultimately how to conduct the analysis.\nThis course will involve a combination of lectures and problem sets. Problem sets will contain a mixture of both theoretical and applied questions and serve to reinforce key concepts and allow students to assess their progress and understanding throughout the course. Assignments will involve analysis of data using the  programming language. This is a free and open source language for statistical computing that is used extensively for data analysis in many fields. Prior experience with the fundamentals of  programming is required."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course is the second in the political science graduate methodology sequence. Completing the introductory course prior to this sequence should prepare you for the material in this class. We will rely on some background knowledge of core concepts in probability, statistics and inference as well as experience with statistical programming in . However, there are no strict, specific course pre-requisites as many different disciplines and departments offer introductory statistics classes that cover the relevant material.\nIn general, you should have had some introduction to probability theory and should be familiar with concepts like the properties of random variables (especially expectation and variance), estimands and estimators, and statistical inference. Familiarity with linear regression is also a plus, but we will be reviewing it during the relevant week.\nPlease contact the instructor if you are interested in enrolling but are unsure of the requirements."
  },
  {
    "objectID": "syllabus.html#logistics",
    "href": "syllabus.html#logistics",
    "title": "Syllabus",
    "section": "Logistics",
    "text": "Logistics\n\nLectures: Mondays/Wednesdays from 9:30am-10:45am\n\nYou should attend lectures regularly as they comprise a significant element of the course instruction. Lecture materials will be posted on the course website.\n\nDiscussion Forum: We will be using Ed as our primary course discussion platform. If you are enrolled in the class on Canvas, you should already have access to the Ed board for this class. Please use this to post questions about the readings/lecture material as well as about the problem sets.\nCourse Materials: Lecture materials, problem sets and tutorial code will be posted on the course website. Problem set solutions will be posted after the due date on Canvas. Links to readings can be found on the course website organized by week."
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThe course will involve readings from a variety of different textbooks and published papers. The class will not require the purchase of a single, specific, text and all excerpts from textbooks are available online (either directly or through library resources). However, you may wish to obtain some of these texts to use as a personal reference and they may be valuable to you in the future.\n\nRecommended Textbooks\n\nImbens, Guido W. and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press. 2010.\nHernán, Miguel A. and James M. Robins. Causal Inference: What If. Chapman & Hall/CRC. 2020.\nMorgan, Stephen L., and Christopher Winship. Counterfactuals and Causal Inference. Cambridge University Press, 2015.\nCunningham, Scott. Causal inference: The Mixtape. Yale University Press, 2021.\nHuntington-Klein, Nick. The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC, 2021.\nWager, Stefan. Causal inference: A statistical learning approach. Draft available here"
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nStudents’ final grades are based on four components:\n\nProblem Sets (25%)\nStudents will complete a total of five problem sets throughout the quarter. Problem sets will cover roughly a two-week period of course material. A complete schedule of the assignments can be found on the Assignments page.\nThe goal of the problem sets is to encourage exploration of the material and to provide you with a clear and credible means of assessing your understanding and progress through the course. As such, problem sets are designed to be challenging and we expect students to find some questions difficult.\nProblem sets will be graded on a (+/✓/-) scale:\n\n+ : Complete and near-perfect work\n✓ : Generally good work with clear effort shown but with notable errors\n- : Significantly incomplete work with major conceptual errors and little effort shown\n\n\nCollaboration Policy\nWe strongly encourage collaboration between students on the problem sets and highly recommend that students discuss problems with each other either in person or via the discussion forum. However, each student is expected to submit their own write-up of the answers and any relevant code.\n\n\nOffice Hours and Online Discussion\nStudents should feel free to discuss any questions about the problem sets with the teaching staff during sections and office hours. We also strongly encourage students to post questions about both the problem sets and the assigned readings on the course discussion board and respond to other students’ questions. Responding to other students’ questions will contribute to your participation grade.\n\n\nSubmission Guidelines\nProblem sets will be distributed as HTMl and Quarto files (.qmd). You should submit your answers and any relevant R code in the same format: including the Quarto file (.qmd extension) and a corresponding rendered .html file as your submission. You will be submitting your problem sets via Gradescope.\n\n\n\nIn-person Midterm (30%)\nWe will have an in-person midterm examination on Wednesday, March 4th, 2026. This exam will cover the material in the first half of the course (experiments and selection-on-observables) The exam will take the form of a standard pen-and-paper timed examination involving both theory and practical analysis of sample code and results.\n\n\nIn-person Final (35%)\nThe final exam will take place in person during exams week. We have been assigned an exam date of May 4, 2026 at 5:05PM - 7:05PM. Room information will be made available nearer to the end of the semester. The exam will take the same form as the midterm, but will be slightly longer given the additional time available during the exam period.\n\n\nParticipation (10%)\nWe expect students to take an active role in learning in both lecture and section. Engagement with the teaching staff by asking and answering questions will contribute to this grade as will interaction on the discussion board."
  },
  {
    "objectID": "syllabus.html#computing",
    "href": "syllabus.html#computing",
    "title": "Syllabus",
    "section": "Computing",
    "text": "Computing\nThis course will use the R programming language. This is a free and open source programming language that is available for nearly all computing platforms. You should download and install it from https://www.r-project.org.\nUnless you have strong preferences for a specific coding environment, we recommend that you use the free RStudio Desktop Integrated Development Environment (IDE) which you can download from https://rstudio.com/products/rstudio/download/#download.\nIn addition to base R, we will be frequently using data management and processing tools found in the tidyverse set of packages along with basic graphics and visualization using ggplot2.\nSee the Resources page for additional information."
  },
  {
    "objectID": "syllabus.html#policy-on-generative-large-language-models",
    "href": "syllabus.html#policy-on-generative-large-language-models",
    "title": "Syllabus",
    "section": "Policy on Generative Large Language Models",
    "text": "Policy on Generative Large Language Models\nLarge Language Models (LLMs) continue to have an immense impact on the educational field. Over the last several years, we have seen striking growth in the capabilities of these models and it is clear that they will remain a permanent and inextricable presence in all of our lives. This very course website has been built from my earlier .tex syllabus with considerable assistance from Claude’s Opus 4.5 model. Having taught a causal inference course every year since 2020, I have seen first-hand how these tools are rapidly reshaping how students engage with the material - for better and for worse.\nThe clearest setting where LLM tools have been amplifying user productivity is in programming. For me, the most positive case for LLMs is that they are a kind of “universal interface” in natural language that makes it easier for users to get a computer to do what they want simply by articulating the task in English. Cutting-edge LLMs implement some form of “reasoning” (essentially, having the model generate intermediate tokens into its context window to improve the quality of the final predicted tokens) and models are able to call various software tools as part of the generation process. Modern LLMs are increasingly “agentic” in that they replicate the sort of think-decide-act loop that we would consider to be the role of a typical knowledge worker. In essence, every graduate student can now have their own team of RAs for a comparatively low monthly payment to Anthropic.\nUnfortunately, the effectiveness of LLMs remains highly variable across different task domains. My own view is that coding (especially coding for software development) is an optimal use case for LLMs because it is an engineering task. By “engineering,” I mean the the iterative task of solving a problem by brainstorming potential solutions, implementing those solutions, and then subsequently evaluating the solutions with respect to some clearly defined criteria. This is because the goal of engineering is the building of systems. The key components here are both the existence of a well-defined problem and the ability to assess whether the proposed solutions are effective. Indeed, modern agentic LLMs are good in large part because they incorporate a lot of this evaluation by way of the “reasoning” structure and have the ability to write unit tests to check the code. Moreover, software development is amenable to this “try-and-check” sort of reasoning because the sub-tasks typically involve some well-defined, structured output that can be evaluated against what the user knows what they need/want.\nProblems begin to accumulate when there is no longer an agent (human or LLM) that can diagnose errors and adjust. Unfortunately, this is the sort of thing that experienced users are better at than less experienced users. Without an understanding of what the LLM is actually doing, you are much less likely to be able to figure out when it has gone wrong. And the fail cases for LLMs are, in my experience, a lot weirder than the usual fail cases for humans. So my most negative assessment of LLMs is that they have severely adversely affected the process of education. While they may supercharge the productivity of those who already have the relevant understanding, they are much less promising in training novices and provide only the illusion of capability.\nI am increasingly opposed to the use of LLMs for programming when one is learning to code. In practice, I find that students delegate far too much to the model and spend insufficient time understanding the mechanics of what the code is doing. This makes it actually quite difficult to meaningfully debug outputs and understand how to diagnose errors. Additionally, actually implementing statistical methods in code is a common way for students to actually understand conceptually what the methods do and how they work. As such, I would still discourage LLM-assistance when completing problem sets although I do not strictly rule it out, especially for the more tedious components where you are confident that you understand what the code is doing (e.g. data cleaning).\nAdditionally, the sort of coding that we do as scientists is typically not software development, so I actually don’t think these tools are (yet) good for writing research code. The task of science I think is distinct from engineering in that we do not have a “try-check-fail-repeat” evaluation loop where the “correctness” of the method and implementation can be assessed from the “correctness” of the output. In fact, we would probably characterize scientists that do this sort of thing as engaging in “questionable research practices” if “correctness” is judged by the “number of publications” objective function. We are not building, we are discovering. A core theme of this class is the importance of outside information and deep knowledge in assessing the feasibility of the assumptions behind a particular research design. The quality of research is determined not by its outputs but rather by its inputs. These are not assumptions that an LLM (for the most part) will provide to you unprompted. An LLM agent can effectively replicate an existing analysis, but it is unlikely to tell you that this was the wrong analysis to begin with. I like to think of LLM agents as amateur improvisers - they can “Yes, and…” you very well, but are much less capable of the “No, but…”\nOne other common use of LLMs that I have also seen - and perhaps this is more common than direct assistance on assignments - is the use of them as “personal tutors.” The chat interfaces let students ask questions, summarize complex texts, search for other relevant information, and brainstorm (see the above point re: “amateur improviser”) without the need for awkward conversations with other human beings. While I think this is fine for simple questions, especially when the LLM just replaces web search, I would not use LLMs as a complete substitute for your colleagues and for the teaching staff. One of the unfortunate consequences of LLM-proliferation is that students don’t post on discussion boards as often as they used to - even in graduate classes. I feel that this is ultimately detrimental to the sort of community-building and professionalization that this class is designed for. Additionally, one of the benefits of asking the teaching staff is that they are familiar enough with the topic and the context that they can infer a lot of what is unstated or implied by your question and better tailor the response. We will provide additional context unprompted.\nWith regards to web-search, I would have strongly advised against this a few years ago, but the incorporation of retrieval augmented generation in modern models has reduced the prevalence of entirely made up sources, though not entirely. Since modern web search runs on basically the same underlying vector search architecture, there’s basically no difference in using an LLM vs. just Googling something. I would still encourage going to the retrieved sources directly though and avoiding the generated summaries since those seem to use cheaper/low-quality models. For summaries, tools explicitly built for interacting with a text (e.g. Google’s NotebookLM) I think are potentially more useful, though I have not spent much time exploring them. Even then, I would still encourage the “classical” approach of searching past (and, with Google Scholar, future) citations. Even if LLMs generated perfect summaries, I think there is an inherent benefit of the “old ways” when it comes to professionalization and developing a better understanding of the authors in a given literature. Unlike the rest of the internet, academia is still a link(citation)-based culture and it is worth leveraging that researcher-provided context to guide your reading. Additionally, these authors are real human beings who you will meet at conferences - it’s worth understanding who they are conversing with in order to better understand the discipline.\nLastly, any LLM policy needs to consider its feasibility. It is clear to me that any restrictions on LLM use aside from restricted, in-class pen-and-paper examination use are fundamentally unenforceable. Therefore, with respect to the problem sets, students are permitted to use LLMs in whatever capacity they see fit. I have attempted to design the problem sets such that they contain “out-of-distribution” challenges (e.g. a replication of an existing paper that concludes contrary to the original result) and otherwise general “traps” that try to evaluate deep substantive knowledge of the problem. Over the last two years, I have found on (e.g. take home exams) that LLM-using students produce mediocre but not completely terrible results. Nevertheless, they do make mistakes (and often behave in ways that could be described as “not wrong, just strange”) and it is clear to me which students use them to their detriment. Perhaps this will change in the next year or two - such is the nature of this field. Indeed, my decision to move entirely to in-person assessment was driven by the observation that although take-home exams still provided some variation among students, that variation was dramatically lower than the in-class exams. So in short:\n\nYou may use them.\nI discourage it for anything important.\nI’ll know if you use them poorly.\nIgnore the problem sets at your own peril. The consequences will arrive at exam time."
  },
  {
    "objectID": "syllabus.html#accommodations-and-accessibility",
    "href": "syllabus.html#accommodations-and-accessibility",
    "title": "Syllabus",
    "section": "Accommodations and Accessibility",
    "text": "Accommodations and Accessibility\nThe University of Wisconsin–Madison supports the right of all enrolled students to a full and equal educational opportunity. The Americans with Disabilities Act (ADA), Wisconsin State Statute (36.12), and UW–Madison policy (Faculty Document 1071) require that students with disabilities be reasonably accommodated in instruction and campus life. Reasonable accommodations for students with disabilities is a shared faculty and student responsibility.\nStudents are expected to inform faculty of their need for instructional accommodations by the end of the third week of the semester, or as soon as possible after a disability has been incurred or recognized.\nI will work either directly with you or in coordination with the McBurney Disability Resource Center to identify and provide reasonable instructional accommodations. Once you are approved for accommodations by the McBurney Center, please be sure to make the relevant selections in McBurney Connect. When I have received your Student Accommodation Letter, I will send you a follow-up e-mail to connect and discuss how the accommodations will be implemented for this course. Disability information, including instructional accommodations as part of a student’s educational record, is confidential and protected under FERPA."
  },
  {
    "objectID": "syllabus.html#acknowledgments",
    "href": "syllabus.html#acknowledgments",
    "title": "Syllabus",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis course is indebted to the many wonderful and generous scholars who have developed causal inference curricula in political science departments throughout the world and who have made their course materials available to the public. In particular, I thank Matthew Blackwell, Brandon Stewart, Molly Roberts, Kosuke Imai, Teppei Yamamoto, Jens Hainmueller, Adam Glynn, Gary King, Justin Grimmer, and Edward Kennedy whose lecture notes and syllabi have been immensely valuable in the creation of this course. Special thanks to Molly Offer-Westort, Andy Eggers and Bobby Gulotty who helped in the development of this course in its earlier incarnation as PLSC 30600 at the University of Chicago. I also thank the previous teaching assistants of this course: Arthur Yu, Oscar Cuadros, Zikai Li, and Cindy Wang.\nLastly, thanks to Andrew Heiss and Matt Blackwell for their Quarto website templates, which I have extensively borrowed from in designing this course site."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "A schedule of topics and readings is provided below. Each week will cover a single topic or group of topics. Monday lectures will typically be an introduction to the topic while Wednesday lectures will go into greater detail and involve some applications of the method. You should make sure to review the readings prior to that week’s lectures with an aim towards completing the reading assignments prior to Wednesday’s lecture.\nApplications are optional but useful examples of a given methodological approach implemented in a published political science (or adjacent discipline) paper. I have tried to curate some examples for each week that provide a contrast between an “older” vs. a more “modern” paper using a particular methodology, highlight a debate over an interesting empirical question that hinges on a methodological problem, and/or just generally provide a cool empirical application using the framework being taught that week.\nAll hyperlinks to papers are either to the published version of the paper (if published) or to the working paper. You should use your university library access to obtain the full version if the article is not open access. Likewise, textbook readings will be to the free/open version of the book (if available) or to the UW-Madison university library eBook."
  },
  {
    "objectID": "schedule.html#week-1-statistical-review",
    "href": "schedule.html#week-1-statistical-review",
    "title": "Schedule",
    "section": "Week 1: Statistical Review",
    "text": "Week 1: Statistical Review\nWednesday, January 21\nTopics:\n\nEstimands and estimators\nReview of statistical properties of estimators (bias/variance)\nLarge sample theory, big-O/little-o notation\n\nReadings:\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review 86 (3): 532–565.\nTorreblanca, Carolina, William Dinneen, Guy Grossman, and Yiqing Xu. 2025. “The Credibility Revolution in Political Science.” Working paper.\nSamii, Cyrus. 2023. “The ‘Problem Solving’ Approach and Social Science Methodology.” Blog post.\nChapter 3; Blackwell, Matthew. 2025. A User’s Guide to Statistical Inference and Regression."
  },
  {
    "objectID": "schedule.html#week-2-potential-outcomes-randomized-experiments",
    "href": "schedule.html#week-2-potential-outcomes-randomized-experiments",
    "title": "Schedule",
    "section": "Week 2: Potential Outcomes + Randomized Experiments",
    "text": "Week 2: Potential Outcomes + Randomized Experiments\nMonday, January 26 & Wednesday, January 28\nTopics:\n\nCounterfactual reasoning and the potential outcomes model\nWhat assumptions are needed to identify average treatment effects\nWhy randomized experiments satisfy these assumptions\nEstimation and randomization inference in standard experimental designs\n\nReadings:\n\nChapter 1; Imbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge: Cambridge University Press.\nChapters 1–2; Hernán, Miguel A., and James M. Robins. 2020. Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\nSections 1–5; Athey, Susan, and Guido W. Imbens. 2017. “The Econometrics of Randomized Experiments.” In Handbook of Economic Field Experiments, vol. 1, edited by Abhijit Vinayak Banerjee and Esther Duflo, 73–140. Amsterdam: North-Holland.\nDruckman, James N., Donald P. Green, James H. Kuklinski, and Arthur Lupia. 2006. “The Growth and Development of Experimental Research in Political Science.” American Political Science Review 100 (4): 627–635.\n\nApplications:\n\nGerber, Alan S., Donald P. Green, and Christopher W. Larimer. 2008. “Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment.” American Political Science Review 102 (1): 33–48.\nBroockman, David E., and Joshua L. Kalla. 2025. “Consuming Cross-Cutting Media Causes Learning and Moderates Attitudes: A Field Experiment with Fox News Viewers.” Journal of Politics 87 (1): 246–261."
  },
  {
    "objectID": "schedule.html#week-3-experiments-attrition-and-generalizability",
    "href": "schedule.html#week-3-experiments-attrition-and-generalizability",
    "title": "Schedule",
    "section": "Week 3: Experiments: Attrition and Generalizability",
    "text": "Week 3: Experiments: Attrition and Generalizability\nMonday, February 2 & Wednesday, February 4\nTopics:\n\nWhat do we do when things happen after the experiment (e.g. attrition)?\nBounding treatment effects and partial identification.\nGeneralizing experimental results beyond the sample ATE\n\nReadings:\n\nEgami, Naoki, and Erin Hartman. 2023. “Elements of External Validity: Framework, Design, and Analysis.” American Political Science Review 117 (3): 1070–1088.\nZhang, Junni L., and Donald B. Rubin. 2003. “Estimation of Causal Effects via Principal Stratification When Some Outcomes Are Truncated by ‘Death.’” Journal of Educational and Behavioral Statistics 28 (4): 353–368.\nLee, David S. 2009. “Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects.” Review of Economic Studies 76 (3): 1071–1102.\n\nApplications:\n\nBassan-Nygate, Lotem, Jonathan Renshon, Jessica L. P. Weeks, and Chagai M. Weiss. 2024. “The Generalizability of IR Experiments beyond the United States.” American Political Science Review 118 (4): 1–16.\nDunning, Thad, Guy Grossman, Macartan Humphreys, Susan D. Hyde, Craig McIntosh, Gareth Nellis, Claire L. Adida, et al. 2019. “Voter Information Campaigns and Political Accountability: Cumulative Findings from a Preregistered Meta-Analysis of Coordinated Trials.” Science Advances 5 (7): eaaw2612."
  },
  {
    "objectID": "schedule.html#week-4-experiments-incorporating-covariates",
    "href": "schedule.html#week-4-experiments-incorporating-covariates",
    "title": "Schedule",
    "section": "Week 4: Experiments: Incorporating Covariates",
    "text": "Week 4: Experiments: Incorporating Covariates\nMonday, February 9 & Wednesday, February 11\nTopics:\n\nStratification/blocking and using covariates in experiments\nAnalysis of cluster-randomized experiments\nIntroduction to regression estimators\n\nReadings:\n\nSections 6–12; Athey, Susan, and Guido W. Imbens. 2017. “The Econometrics of Randomized Experiments.” In Handbook of Economic Field Experiments, vol. 1, edited by Abhijit Vinayak Banerjee and Esther Duflo, 73–140. Amsterdam: North-Holland.\nChapters 5–7; Blackwell, Matthew. 2025. A User’s Guide to Statistical Inference and Regression.\nChapter 7; Imbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge: Cambridge University Press.\nLin, Winston. 2013. “Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique.” Annals of Applied Statistics 7 (1): 295–318.\nBonus: Samii, Cyrus, and P. M. Aronow. 2012. “On Equivalencies between Design-Based and Regression-Based Variance Estimators for Randomized Experiments.” Statistics & Probability Letters 82 (2): 365–370.\n\nApplications:\n\nNyhan, Brendan, and Jason Reifler. 2015. “The Effect of Fact-Checking on Elites: A Field Experiment on US State Legislators.” American Journal of Political Science 59 (3): 628–640.\nRaffler, Pia J. 2022. “Does Political Oversight of the Bureaucracy Increase Accountability? Field Experimental Evidence from a Dominant Party Regime.” American Political Science Review 116 (4): 1443–1459."
  },
  {
    "objectID": "schedule.html#week-5-selection-on-observables",
    "href": "schedule.html#week-5-selection-on-observables",
    "title": "Schedule",
    "section": "Week 5: Selection-on-Observables",
    "text": "Week 5: Selection-on-Observables\nMonday, February 16 & Wednesday, February 18\nTopics:\n\nWhat to do when random assignment of treatment is not possible - common challenges of observational designs\nAssumptions behind “no unobserved confounding” designs\nRepresenting assumptions using graphical models\nCovariate adjustment using stratification\n\nReadings:\n\nChapter 12; Imbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge: Cambridge University Press.\nChapters 3, 6–8; Hernán, Miguel A., and James M. Robins. 2020. Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\nChapters 6–8; Huntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. Boca Raton: Chapman and Hall/CRC.\nSupplementary: Cinelli, Carlos, Andrew Forney, and Judea Pearl. 2024. “A Crash Course in Good and Bad Controls.” Sociological Methods & Research 53 (3): 1071–1104.\nSupplementary: Rohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42.\n\nApplications:\n\nWashington, Ebonya L. 2008. “Female Socialization: How Daughters Affect Their Legislator Fathers.” American Economic Review 98 (1): 311–332.\nBa, Bocar A., Dean Knox, Jonathan Mummolo, and Roman Rivera. 2021. “The Role of Officer Race and Gender in Police-Civilian Interactions in Chicago.” Science 371 (6530): 696–702."
  },
  {
    "objectID": "schedule.html#week-6-selection-on-observables---regression-and-weighting",
    "href": "schedule.html#week-6-selection-on-observables---regression-and-weighting",
    "title": "Schedule",
    "section": "Week 6: Selection-on-Observables - Regression and Weighting",
    "text": "Week 6: Selection-on-Observables - Regression and Weighting\nMonday, February 23 & Wednesday, February 25\nTopics:\n\nCovariate adjustment using regression estimators\nPropensity scores and covariate adjustment via weighting\nCombining treatment and outcome models for a “doubly-robust” estimator\n\nReadings:\n\nImbens, Guido W. 2004. “Nonparametric Estimation of Average Treatment Effects under Exogeneity: A Review.” Review of Economics and Statistics 86 (1): 4–29.\nGlynn, Adam N., and Kevin M. Quinn. 2010. “An Introduction to the Augmented Inverse Propensity Weighted Estimator.” Political Analysis 18 (1): 36–56.\nAronow, P. M., and Cyrus Samii. 2016. “Does Regression Produce Representative Estimates of Causal Effects?” American Journal of Political Science 60 (1): 250–267.\nChapters 11–12; Hernán, Miguel A., and James M. Robins. 2020. Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\n\nApplications:\n\nHübert, Ryan, and Ryan Copus. 2022. “Political Appointments and Outcomes in Federal District Courts.” Journal of Politics 84 (2): 908–922.\nJares, Jake Alton, and Neil Malhotra. 2025. “Policy Impact and Voter Mobilization: Evidence from Farmers’ Trade War Experiences.” American Political Science Review 119 (2): 847–869."
  },
  {
    "objectID": "schedule.html#week-7-selection-on-observables---matching",
    "href": "schedule.html#week-7-selection-on-observables---matching",
    "title": "Schedule",
    "section": "Week 7: Selection-on-Observables - Matching",
    "text": "Week 7: Selection-on-Observables - Matching\nMonday, March 2 & Wednesday, March 4\nTopics:\n\nCovariate adjustment using matching estimators\nMidterm exam on Wednesday March 4th\n\nReadings:\n\nRosenbaum, Paul R. 2020. “Modern Algorithms for Matching in Observational Studies.” Annual Review of Statistics and Its Application 7 (1): 143–176.\nAbadie, Alberto, and Guido W. Imbens. 2011. “Bias-Corrected Matching Estimators for Average Treatment Effects.” Journal of Business & Economic Statistics 29 (1): 1–11.\nImbens, Guido W., and Yiqing Xu. 2025. “Comparing Experimental and Nonexperimental Methods: What Lessons Have We Learned Four Decades after LaLonde (1986)?” Journal of Economic Perspectives 39 (4): 173–201.\n\nApplications:\n\nImai, Kosuke. 2005. “Do Get-Out-the-Vote Calls Reduce Turnout? The Importance of Statistical Methods for Field Experiments.” American Political Science Review 99 (2): 283–300.\nHansen, Ben B., and Jake Bowers. 2009. “Attributing Effects to a Cluster-Randomized Get-Out-the-Vote Campaign.” Journal of the American Statistical Association 104 (487): 873–885."
  },
  {
    "objectID": "schedule.html#week-8-instrumental-variables",
    "href": "schedule.html#week-8-instrumental-variables",
    "title": "Schedule",
    "section": "Week 8: Instrumental Variables",
    "text": "Week 8: Instrumental Variables\nMonday, March 9 & Wednesday, March 11\nTopics:\n\nIdentifying causal effects under unobserved confounding using exogenous variation in treatment induced by an instrument\nEstimation via the Wald estimator and two stage least squares (TSLS)\nInterpreting the IV estimand - the LATE theorem\n\nReadings:\n\nChapter 7; Cunningham, Scott. 2021. Causal Inference: The Mixtape. New Haven: Yale University Press.\nAngrist, Joshua D., Guido W. Imbens, and Donald B. Rubin. 1996. “Identification of Causal Effects Using Instrumental Variables.” Journal of the American Statistical Association 91 (434): 444–455.\nAndrews, Isaiah, James H. Stock, and Liyang Sun. 2019. “Weak Instruments in Instrumental Variables Regression: Theory and Practice.” Annual Review of Economics 11: 727–753.\nSłoczyński, Tymon. 2024. “When Should We (Not) Interpret Linear IV Estimands as LATE?” Working paper.\n\nApplications:\n\nWhite, Ariel. 2019. “Misdemeanor Disenfranchisement? The Demobilizing Effects of Brief Jail Spells on Potential Voters.” American Political Science Review 113 (2): 311–324.\nLal, Apoorva, Mackenzie Lockhart, Yiqing Xu, and Ziwen Zu. 2024. “How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on 67 Replicated Studies.” Political Analysis 32 (4): 521–540.\nAngrist, Joshua D., and Peter Hull. 2023. “Instrumental Variables Methods Reconcile Intention-to-Screen Effects across Pragmatic Cancer Screening Trials.” Proceedings of the National Academy of Sciences 120 (51): e2311556120."
  },
  {
    "objectID": "schedule.html#week-9-differences-in-differences",
    "href": "schedule.html#week-9-differences-in-differences",
    "title": "Schedule",
    "section": "Week 9: Differences-in-Differences",
    "text": "Week 9: Differences-in-Differences\nMonday, March 16 & Wednesday, March 18\nTopics:\n\nLeveraging repeated outcomes over time to address unobserved confounding\nAssumptions behind the “differences-in-differences” strategy – parallel trends\nEstimation and diagnostics for the identification assumptions\nConnection to regression estimators with “two-way fixed effects”\n\nReadings:\n\nBaker, Andrew, Brantly Callaway, Scott Cunningham, Andrew Goodman-Bacon, and Pedro H. C. Sant’Anna. 2025. “Difference-in-Differences Designs: A Practitioner’s Guide.” Working paper.\nChapter 9; Cunningham, Scott. 2021. Causal Inference: The Mixtape. New Haven: Yale University Press.\nChapter 3; de Chaisemartin, Clément, and Xavier D’Haultfœuille. 2023. Credible Answers to Hard Questions: Differences-in-Differences for Natural Experiments. Working paper.\n\nApplications:\n\nCard, David, and Alan B. Krueger. 1994. “Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania.” American Economic Review 84 (4): 772–793.\nMiller, Sarah, Norman Johnson, and Laura R. Wherry. 2021. “Medicaid and Mortality: New Evidence from Linked Survey and Administrative Data.” Quarterly Journal of Economics 136 (3): 1783–1829."
  },
  {
    "objectID": "schedule.html#week-10-modern-differences-in-differences",
    "href": "schedule.html#week-10-modern-differences-in-differences",
    "title": "Schedule",
    "section": "Week 10: Modern Differences-in-Differences",
    "text": "Week 10: Modern Differences-in-Differences\nMonday, March 23 & Wednesday, March 25\nTopics:\n\nDifferences-in-differences under staggered adoption\n“New” DiD estimators - TWFE regression imputation vs. AIPW\nDiagnosing pre-trends violations and assessing robustness\nRelaxing parallel trends assumptions\n\nReadings:\n\nRoth, Jonathan, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–2244.\nLiu, Licheng, Ye Wang, and Yiqing Xu. 2024. “A Practical Guide to Counterfactual Estimators for Causal Inference with Time-Series Cross-Sectional Data.” American Journal of Political Science 68 (1): 160–176.\nCaetano, Carolina, and Brantly Callaway. 2024. “Difference-in-Differences When Parallel Trends Holds Conditional on Covariates.” Working paper.\nRambachan, Ashesh, and Jonathan Roth. 2023. “A More Credible Approach to Parallel Trends.” Review of Economic Studies 90 (5): 2555–2591.\nStrezhnev, Anton. 2024. “Group-Specific Linear Trends and the Triple-Differences in Time Design.” Working paper.\n\nApplications:\n\nChiu, Albert, Xingchen Lan, Ziyi Liu, and Yiqing Xu. 2023. “Causal Panel Analysis under Parallel Trends: Lessons from a Large Reanalysis Study.” American Political Science Review 118 (4): 1–22."
  },
  {
    "objectID": "schedule.html#spring-break",
    "href": "schedule.html#spring-break",
    "title": "Schedule",
    "section": "Spring Break",
    "text": "Spring Break\nNo Class: Monday, March 23 & Wednesday, March 25"
  },
  {
    "objectID": "schedule.html#week-11-panel-data-causal-inference",
    "href": "schedule.html#week-11-panel-data-causal-inference",
    "title": "Schedule",
    "section": "Week 11: Panel Data Causal Inference",
    "text": "Week 11: Panel Data Causal Inference\nMonday, April 6 & Wednesday, April 8\nTopics:\n\nEstimating effects when past outcomes affect future treatments\nEstimators for effects of treatment histories\nPitfalls and cautions with lagged outcome regressions\n\nReadings:\n\nBlackwell, Matthew, and Adam N. Glynn. 2018. “How to Make Causal Inferences with Time-Series Cross-Sectional Data under Selection on Observables.” American Political Science Review 112 (4): 1067–1082.\nXu, Yiqing. 2023. “Causal Inference with Time-Series Cross-Sectional Data: A Reflection.” In The SAGE Handbook of Research Methods in Political Science and International Relations, edited by Luigi Curini and Robert Franzese. London: SAGE.\nDaw, Jamie R., and Laura A. Hatfield. 2018. “Matching and Regression to the Mean in Difference-in-Differences Analysis.” Health Services Research 53 (6): 4138–4156.\n\nApplications:\n\nHarvey, Cole J. 2022. “Who Delivers the Votes? Elected versus Appointed Local Executives, Election Manipulation, and Natural Support for Ruling Parties.” Electoral Studies 76: 102455.\nBach, Laurent, Antoine Bozio, Arthur Guillouzouic, and Clément Malgouyres. 2023. “Dividend Taxes and the Allocation of Capital: Comment.” American Economic Review 113 (7): 2048–2052."
  },
  {
    "objectID": "schedule.html#week-12-regression-discontinuity-designs",
    "href": "schedule.html#week-12-regression-discontinuity-designs",
    "title": "Schedule",
    "section": "Week 12: Regression Discontinuity Designs",
    "text": "Week 12: Regression Discontinuity Designs\nMonday, April 13 & Wednesday, April 15\nTopics:\n\nIdentification under unobserved confounding using quasi-random assignment at a cutpoint\nLocal randomization vs. continuity-based approaches\nEstimation using local polynomial regression\nDiagnostics for design assumptions - “bunching” tests and placebo outcomes\n\nReadings:\n\nCattaneo, Matias D., Nicolás Idrobo, and Rocío Titiunik. 2019. A Practical Introduction to Regression Discontinuity Designs: Foundations. Cambridge: Cambridge University Press.\nCattaneo, Matias D., Nicolás Idrobo, and Rocío Titiunik. 2024. A Practical Introduction to Regression Discontinuity Designs: Extensions. Cambridge: Cambridge University Press.\nMarshall, John. 2024. “Can Close Election Regression Discontinuity Designs Identify Effects of Winning Politician Characteristics?” American Journal of Political Science 68 (2): 494–510.\nHausman, Catherine, and David S. Rapson. 2018. “Regression Discontinuity in Time: Considerations for Empirical Applications.” Annual Review of Resource Economics 10 (1): 533–552.\n\nApplications:\n\nCaughey, Devin, and Jasjeet S. Sekhon. 2011. “Elections and the Regression Discontinuity Design: Lessons from Close US House Races, 1942–2008.” Political Analysis 19 (4): 385–408.\nEggers, Andrew C., Anthony Fowler, Jens Hainmueller, Andrew B. Hall, and James M. Snyder Jr. 2015. “On the Validity of the Regression Discontinuity Design for Estimating Electoral Effects: New Evidence from Over 40,000 Close Races.” American Journal of Political Science 59 (1): 259–274.\nDe la Cuesta, Brandon, and Kosuke Imai. 2016. “Misunderstandings about the Regression Discontinuity Design in the Study of Close Elections.” Annual Review of Political Science 19 (1): 375–396."
  },
  {
    "objectID": "schedule.html#week-13-mediation-and-sensitivity-analysis",
    "href": "schedule.html#week-13-mediation-and-sensitivity-analysis",
    "title": "Schedule",
    "section": "Week 13: Mediation and Sensitivity Analysis",
    "text": "Week 13: Mediation and Sensitivity Analysis\nMonday, April 20 & Wednesday, April 22\nTopics:\n\nAssessing the robustness of selection-on-observables results to hypothetical unobserved confounding.\nDecomposing causal effects into “direct” and “indirect” components\nEstimating mediation effects under ignorability assumptions\nChallenges and pitfalls of mediation analysis\n\nReadings:\n\nCinelli, Carlos, and Chad Hazlett. 2020. “Making Sense of Sensitivity: Extending Omitted Variable Bias.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82 (1): 39–67.\nImai, Kosuke, Luke Keele, Dustin Tingley, and Teppei Yamamoto. 2011. “Unpacking the Black Box of Causality: Learning about Causal Mechanisms from Experimental and Observational Studies.” American Political Science Review 105 (4): 765–789.\nBonus: Green, Donald P., Shang E. Ha, and John G. Bullock. 2010. “Enough Already about ‘Black Box’ Experiments: Studying Mediation Is More Difficult than Most Scholars Suppose.” Annals of the American Academy of Political and Social Science 628 (1): 200–208.\nAcharya, Avidit, Matthew Blackwell, and Maya Sen. 2016. “Explaining Causal Findings without Bias: Detecting and Assessing Direct Effects.” American Political Science Review 110 (3): 512–529.\nBlackwell, Matthew, Ruofan Ma, and Aleksei Opacic. 2024. “Assumption Smuggling in Intermediate Outcome Tests of Causal Mechanisms.” Working paper.\n\nApplications:\n\nRathbun, Brian C., Christopher Sebastian Parker, and Caleb Pomeroy. 2025. “Separate but Unequal: Ethnocentrism and Racialization Explain the ‘Democratic’ Peace in Public Opinion.” American Political Science Review 119 (2): 621–636.\nTomz, Michael, and Jessica L. P. Weeks. 2025. “Race, Democracy, and Public Support for War.” American Political Science Review: 1–18.\nHazlett, Chad. 2020. “Angry or Weary? How Violence Impacts Attitudes toward Peace among Darfurian Refugees.” Journal of Conflict Resolution 64 (5): 844–870."
  },
  {
    "objectID": "schedule.html#week-14-shift-share-designs-and-experiments-with-interference",
    "href": "schedule.html#week-14-shift-share-designs-and-experiments-with-interference",
    "title": "Schedule",
    "section": "Week 14: Shift-share designs and experiments with interference",
    "text": "Week 14: Shift-share designs and experiments with interference\nMonday, April 27 & Wednesday, April 29\nTopics:\n\nIdentification with instruments that are inner products of unit-specific “shares” and independent global “shifts”\n“Share-exogeneity” and connections to differences-in-differences/“multiple instruments”\n“Shift-exogeneity” and connections to experiments with interference\n\nReadings:\n\nBorusyak, Kirill, Peter Hull, and Xavier Jaravel. 2025. “A Practical Guide to Shift-Share Instruments.” Journal of Economic Perspectives 39 (1): 181–204.\nBorusyak, Kirill, Peter Hull, and Xavier Jaravel. 2025. “Design-Based Identification with Formula Instruments: A Review.” Econometrics Journal 28 (1): 83–108.\nAronow, P. M., and Cyrus Samii. 2017. “Estimating Average Causal Effects under General Interference, with Application to a Social Network Experiment.” Annals of Applied Statistics 11 (4): 1912–1947.\n\nApplications:\n\nFouka, Vasiliki, and Marco Tabellini. 2022. “Changing In-Group Boundaries: The Effect of Immigration on Race Relations in the United States.” American Political Science Review 116 (3): 968–984.\nGulotty, Robert, and Anton Strezhnev. 2024. “The Political Benefits of the Monoculture: Estimating the Electoral Effect of the Market Facilitation Program.” Working paper."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Lecture slides are available as self-contained .html files made with RevealJS from a Quarto .qmd file. You can choose to print these to a PDF document by using the “PDF Export Mode” in the Tools menu accessed via the settings button in the lower left-hand side of the page. You are also welcome to directly download the .qmd file and supporting data/images/etc… from the Github repository. Slides are organized by week and combine Monday and Wednesday’s lectures.\n\n\n\nWeek 1 Github Repository\nWeek 1 .html slides\n\n\n\n\n\nWeek 2 Github Repository\nWeek 2 .html slides\n\n\n\n\n\nWeek 3 Github Repository\nWeek 3 .html slides\n\n\n\n\n\nWeek 4 Github Repository\nWeek 4 .html slides\n\n\n\n\n\nWeek 5 Github Repository\nWeek 5 .html slides\n\n\n\n\n\nWeek 6 Github Repository\nWeek 6 .html slides\n\n\n\n\n\nWeek 7 Github Repository\nWeek 7 .html slides\n\n\n\n\n\nWeek 8 Github Repository\nWeek 8 .html slides\n\n\n\n\n\nWeek 9 Github Repository\nWeek 9 .html slides\n\n\n\n\n\nWeek 10 Github Repository\nWeek 10 .html slides\n\n\n\n\n\nWeek 11 Github Repository\nWeek 11 .html slides\n\n\n\n\n\nWeek 12 Github Repository\nWeek 12 .html slides\n\n\n\n\n\nWeek 13 Github Repository\nWeek 13 .html slides\n\n\n\n\n\nWeek 14 Github Repository\nWeek 14 .html slides"
  },
  {
    "objectID": "lectures.html#lecture-slides",
    "href": "lectures.html#lecture-slides",
    "title": "Lectures",
    "section": "",
    "text": "Lecture slides are available as self-contained .html files made with RevealJS from a Quarto .qmd file. You can choose to print these to a PDF document by using the “PDF Export Mode” in the Tools menu accessed via the settings button in the lower left-hand side of the page. You are also welcome to directly download the .qmd file and supporting data/images/etc… from the Github repository. Slides are organized by week and combine Monday and Wednesday’s lectures.\n\n\n\nWeek 1 Github Repository\nWeek 1 .html slides\n\n\n\n\n\nWeek 2 Github Repository\nWeek 2 .html slides\n\n\n\n\n\nWeek 3 Github Repository\nWeek 3 .html slides\n\n\n\n\n\nWeek 4 Github Repository\nWeek 4 .html slides\n\n\n\n\n\nWeek 5 Github Repository\nWeek 5 .html slides\n\n\n\n\n\nWeek 6 Github Repository\nWeek 6 .html slides\n\n\n\n\n\nWeek 7 Github Repository\nWeek 7 .html slides\n\n\n\n\n\nWeek 8 Github Repository\nWeek 8 .html slides\n\n\n\n\n\nWeek 9 Github Repository\nWeek 9 .html slides\n\n\n\n\n\nWeek 10 Github Repository\nWeek 10 .html slides\n\n\n\n\n\nWeek 11 Github Repository\nWeek 11 .html slides\n\n\n\n\n\nWeek 12 Github Repository\nWeek 12 .html slides\n\n\n\n\n\nWeek 13 Github Repository\nWeek 13 .html slides\n\n\n\n\n\nWeek 14 Github Repository\nWeek 14 .html slides"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nAssigned\nDue\n\n\n\n\nProblem Set 1\nJan 21 (Week 1)\nFeb 4 (Week 3)\n\n\nProblem Set 2\nFeb 4 (Week 3)\nFeb 18 (Week 5)\n\n\nProblem Set 3\nFeb 18 (Week 5)\nMarch 4 (Week 7)\n\n\nMidterm Exam\nMarch 4 (Week 7)\nIn-Class\n\n\nProblem Set 4\nMarch 11 (Week 8)\nMarch 25 (Week 10)\n\n\nProblem Set 5\nApril 8 (Week 11)\nApril 22 (Week 13)\n\n\nFinal Exam\nMay 4 (Exam Week)\nLocation TBA"
  },
  {
    "objectID": "assignments.html#schedule",
    "href": "assignments.html#schedule",
    "title": "Assignments",
    "section": "",
    "text": "Assignment\nAssigned\nDue\n\n\n\n\nProblem Set 1\nJan 21 (Week 1)\nFeb 4 (Week 3)\n\n\nProblem Set 2\nFeb 4 (Week 3)\nFeb 18 (Week 5)\n\n\nProblem Set 3\nFeb 18 (Week 5)\nMarch 4 (Week 7)\n\n\nMidterm Exam\nMarch 4 (Week 7)\nIn-Class\n\n\nProblem Set 4\nMarch 11 (Week 8)\nMarch 25 (Week 10)\n\n\nProblem Set 5\nApril 8 (Week 11)\nApril 22 (Week 13)\n\n\nFinal Exam\nMay 4 (Exam Week)\nLocation TBA"
  },
  {
    "objectID": "assignments.html#problem-sets",
    "href": "assignments.html#problem-sets",
    "title": "Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\nProblem sets are distributed as .qmd files along with their rendered .html form. You should edit the provided .qmd file and render to .html when submitting the assignment. Additional files (e.g. datasets) are located in sub-folders. For convenience, all the combined files are also distributed as a .zip compressed archive.\n\nProblem Set 1\n\nGithub Repository\n.zip archive"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PoliSci 813",
    "section": "",
    "text": "Causal Inference\nDesigning studies to uncover cause and effect from data\nPoliSci 813 • Spring 2026University of Wisconsin-Madison\n\n\n\n\n\n\n\n\nInstructor\n\n   Anton Strezhnev\n   North Hall 322D\n   strezhnev@wisc.edu\n\n\n\nTeaching Assistant\n\n   Junda Li\n   jli2479@wisc.edu\n\n\n\n\nCourse details\n\n   Mondays & Wednesdays\n   1/21/2026 - 5/4/2026\n   9:30am - 10:45am\n   North Hall 422\n\n\n\nContacting me\nE-mail is the best way to get in touch with me. My dedicated office hours are Tuesdays from 2pm-4pm if you want to meet in-person. I am also generally in my office on weekdays during normal hours and my door is open, so you are welcome to drop in. You can also feel free to e-mail me to confirm availability or to schedule a virtual meeting via Zoom."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "These textbooks provide a number of the readings that will be used through the course of the semester. Each is available either as a free online version or in digital form via UW-Madison library access.\n\nCunningham, Scott. Causal inference: The Mixtape. Yale University Press, 2021.\n\nFree online version\n\nHuntington-Klein, Nick. The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC, 2021.\n\nFree online version\n\nHernán, Miguel A. and James M. Robins. Causal Inference: What If. Chapman & Hall/CRC. 2020.\n\nFree PDF available\n\nImbens, Guido W. and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press. 2015.\n\nUW-Madison Library Access\n\nMorgan, Stephen L., and Christopher Winship. Counterfactuals and Causal Inference. Cambridge University Press, 2015.\n\nUW-Madison Library Access\n\nWager, Stefan. Causal inference: A statistical learning approach.\n\nDraft PDF available here"
  },
  {
    "objectID": "resources.html#textbooks",
    "href": "resources.html#textbooks",
    "title": "Resources",
    "section": "",
    "text": "These textbooks provide a number of the readings that will be used through the course of the semester. Each is available either as a free online version or in digital form via UW-Madison library access.\n\nCunningham, Scott. Causal inference: The Mixtape. Yale University Press, 2021.\n\nFree online version\n\nHuntington-Klein, Nick. The Effect: An Introduction to Research Design and Causality. Chapman and Hall/CRC, 2021.\n\nFree online version\n\nHernán, Miguel A. and James M. Robins. Causal Inference: What If. Chapman & Hall/CRC. 2020.\n\nFree PDF available\n\nImbens, Guido W. and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press. 2015.\n\nUW-Madison Library Access\n\nMorgan, Stephen L., and Christopher Winship. Counterfactuals and Causal Inference. Cambridge University Press, 2015.\n\nUW-Madison Library Access\n\nWager, Stefan. Causal inference: A statistical learning approach.\n\nDraft PDF available here"
  },
  {
    "objectID": "resources.html#software",
    "href": "resources.html#software",
    "title": "Resources",
    "section": "Software",
    "text": "Software\n\nR and RStudio\nThis course uses R for all programming assignments. You are welcome to use the RStudio Interactive Development Environment (IDE) to write code and edit the assignment write-up. Due to the extensive integration of Quarto into Posit-developed IDEs, I encourage using either RStudio or the more recent Positron IDE.\n\nR: Download from https://www.r-project.org\nRStudio: Download from https://posit.co/download/rstudio-desktop/\nPositron: Download from https://positron.posit.co/\n\n\nRecommended Guides for R Programming\n\nR For Data Science Available at https://r4ds.hadley.nz/\nData Visualization: A Practical Introduction Available at https://socviz.co/\n\n\n\n\nQuarto and Markdown\nThe assignments are distributed as .qmd Quarto files. Quarto is the successor to R Markdown and lets you present your analysis, code, figures and written discussion all in a single document. It uses Markdown syntax for formatting text and supports embedded R code chunks that execute when you render the document. This lets you present your analysis, code, figures, and written discussion all in one place.\nTo complete assignments, you will edit the provided .qmd file, adding your code and written responses, then render it to an HTML file for submission. Both your .html output and .qmd file will be submitted via Gradescope\n\nQuarto: Download from https://quarto.org/docs/get-started/\nQuarto Guide: https://quarto.org/docs/guide/\nMarkdown Basics: https://quarto.org/docs/authoring/markdown-basics.html\nNote also the chapter on Quarto in R For Data Science"
  },
  {
    "objectID": "staff.html",
    "href": "staff.html",
    "title": "Staff",
    "section": "",
    "text": "Assistant Professor\nDepartment of Political Science\nUniversity of Wisconsin-Madison\n\n\n\n\n\nOffice\nNorth Hall 322D\n\n\nEmail\nstrezhnev@wisc.edu\n\n\nWebsite\nantonstrezhnev.com\n\n\nOffice Hours\nTuesdays, 2pm-4pm (or by appointment via email)"
  },
  {
    "objectID": "staff.html#instructor",
    "href": "staff.html#instructor",
    "title": "Staff",
    "section": "",
    "text": "Assistant Professor\nDepartment of Political Science\nUniversity of Wisconsin-Madison\n\n\n\n\n\nOffice\nNorth Hall 322D\n\n\nEmail\nstrezhnev@wisc.edu\n\n\nWebsite\nantonstrezhnev.com\n\n\nOffice Hours\nTuesdays, 2pm-4pm (or by appointment via email)"
  },
  {
    "objectID": "staff.html#teaching-assistant",
    "href": "staff.html#teaching-assistant",
    "title": "Staff",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\n\nJunda Li\n\n\nPhD Candidate\nDepartment of Political Science\nUniversity of Wisconsin-Madison\n\n\n\n\n\nEmail\njli2458@wisc.edu\n\n\nWebsite\njundali.org\n\n\nOffice Hours\nThursdays, 1:30pm-3:30pm, North Hall 101 (Grad Lounge)"
  },
  {
    "objectID": "staff.html#contact-policy",
    "href": "staff.html#contact-policy",
    "title": "Staff",
    "section": "Contact Policy",
    "text": "Contact Policy\n\nCourse questions: Please post questions about course content on the Ed discussion forum so that all students can benefit from the answers.\nPersonal matters: Email the instructor directly.\nProblem set questions: Post on the discussion forum or come to office hours.\nExpected response time: We aim to respond to discussion posts within 24 hours on weekdays."
  }
]